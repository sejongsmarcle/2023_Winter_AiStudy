# 6 ~ 9 퍼셉트론, 다층 퍼셉트론, 오차 역전파, 신경망에서 딥러닝으로

# 6 퍼셉트론

퍼셉트론은 입력 값과 활성화 함수를 사용해 출력 값을 다음으로 넘기는 가장 작은 신경망 단위이다.

+ 가중치, 가중합, 바이어스 활성화 함수 용어정리

<img width="457" alt="image" src="https://user-images.githubusercontent.com/101916929/215665424-cd3e93d8-f813-4bbe-843b-f0a60174a316.png">

w(weight): 가중치 
b(bias): 바이어스, 편향
가중합: 입력값 x와 가중치w의 곱과 바이어스b값을 더한 결과값

위 가중합 값에 따라 1 or 0을 출력 0과1을 판단하는 함수는 활성화함수.

+ 퍼셉트론의 한계

AND게이트, OR게이트의 경우 선을 그어 결과값을 구분 할 수 있으나 XOR게이트는 선을 그어 구분할 수 없다.

인공지능의 선구자였던 MIT의 마빈 민스키 교수가 1969년 퍼셉트론즈라는 논문에서 위 내용을 밝히고 인공지능 연구의 침체기를 맞음

10여년이 지난후 이 문제가 해결 되었고 그것이 아래 다층 퍼셉트론이다.


***

# 7. 다층 퍼셉트론

다층 퍼셉트론에서는 은닉층이라는 개념이 가장 중요한 개념이라고 생각된다.

입력값만을 놓고 보면 영역 구분이 힘들더라도 은닉층을 만들어 공간을 왜곡하면 두 영역을 나눌 수 있다.

+ 다층 퍼셉트론의 설계

<img width="230" alt="image" src="https://user-images.githubusercontent.com/101916929/215666703-eb4ea919-ee08-4f3c-b820-97f274e5e887.png">


<img width="136" alt="image" src="https://user-images.githubusercontent.com/101916929/215666772-6ac208a6-1d2b-4d9a-a79c-aab8cbd3a9a8.png">

n1, n2는 단일 퍼셉트론과 같다. 
두 식의 결괏값이 출력층으로 보내집니다. 
출력층에서는 활성화 함수를 통해 y값이 결정된다.

<img width="156" alt="image" src="https://user-images.githubusercontent.com/101916929/215667006-76d48fce-0821-4a0e-b946-ae040b487d82.png">


y 값이 결정되었다면 가중치와 바이어스의 값을 정할 차레이다.
2차원 배열로 늘어놓으면 다음과 같다.

<img width="193" alt="image" src="https://user-images.githubusercontent.com/101916929/215667331-406d211d-38ab-4a07-af62-0a885d0be045.png">

***

+ 코딩으로 XOR문제 해결하기

``` python


import numpy as np

# 가중치와 바이어스
w11 = np.array([-2, -2])
w12 = np.array([2, 2])
w2 = np.array([1, 1])
b1 = 3
b2 = -1
b3 = -1

# 퍼셉트론
def MLP(x, w, b):
    y = np.sum(w * x) + b
    if y <= 0:
        return 0
    else:
        return 1

# NAND 게이트
def NAND(x1,x2):
    return MLP(np.array([x1, x2]), w11, b1)

# OR 게이트
def OR(x1,x2):
    return MLP(np.array([x1, x2]), w12, b2)

# AND 게이트
def AND(x1,x2):
    return MLP(np.array([x1, x2]), w2, b3)

# XOR 게이트
def XOR(x1,x2):
    return AND(NAND(x1, x2),OR(x1,x2))


# x1, x2 값을 번갈아 대입해 가며 최종값 출력
if __name__ == '__main__':
    for x in [(0, 0), (1, 0), (0, 1), (1, 1)]:
        y = XOR(x[0], x[1])
        print("입력 값: " + str(x) + " 출력 값: " + str(y))
        
```


# 8. 오차 역전파

+ 오차 역전파의 개념

위에서 XOR문제를 해결하기 위해 신경망을 이용했다.
XOR문제를 해결하기 위해서는 가중치와 바이어스 값을 알아내야한다.
그렇다면 실제 프로젝트에서 어떻게 이 값을 구할까. 
경사 하강법의 확장인 오차 역전파를 이용하자.

<img width="289" alt="image" src="https://user-images.githubusercontent.com/101916929/215668388-2fadfd94-fcf6-43f4-97bb-dcca00eee8ca.png">

+ 임의의 초기 가중치를 준 뒤 결과를 계산한다.
+ 계산 결과와 우리가 원하는 값 사이의 오차를 구한다.
+ 경사 하강법을 이용해 바로 앞 가중치를 오차가 작아지는 방향으로 업데이트한다.
+ 위 과정을 더이상 오차가 줄어들지 않을 때까지 반복한다.

***

코딩으로 확인하는 오차 역전파

<img width="240" alt="image" src="https://user-images.githubusercontent.com/101916929/215668824-bd16cfb8-6dfd-48be-bef7-e9637184229a.png">


+ 환경 변수 지정: 환경 변수에는 입력 값과 타깃 결과값이 포함된 데이터셋, 학습률 등이 포함됩니다. 또한 , 활성화 함수와 가중치 등도 선언되어야 한다.
+ 신경망 실행: 초깃값을 입력하여 활성화 함수와 가중치를 거쳐 결괏값이 나오게 합니다.
+ 결과를 실제 값과 비교 : 오차를 측정한다.
+ 역전파 실행 : 출력층과 은닉층의 가중치를 수정
+ 출력

***

# 9. 신경망에서 딥러닝으로


퍼셉트론 -> 다층 퍼셉트론 -> 오차 역전파, 신경망 => 인공지능 완성 일줄 알았지만 쉽지 않았다.

그 이유와 어떻게 해결했는지를 9장에서 배워보자.

+ 기울기 소실문제와 활성화 함수

층이 늘어나면서 역전파를 통해 전달되는 기울기의 값이 점점 작아져 맨 처음 층까지 전달되지 않는 기울기 소실 문제가 발생하기 시작했다.

<img width="348" alt="image" src="https://user-images.githubusercontent.com/101916929/215669695-0d493417-5785-41a4-b4ab-4ccd47eb0c29.png">

이는 활성화 함수로 사용된 시그모이드 함수의 특성 떄문이다.

여러층을 거칠수록 기울기가 사라져 가중치를 수정하기 어려워졌기 때문이다.

이를 해결하기 위해 다른 여러 활성화 함수로 대체하기 시작했다.

<img width="321" alt="image" src="https://user-images.githubusercontent.com/101916929/215669925-b948a62e-e6e2-4d2e-b730-6cd50ac38648.png">

***

+ 속도와 정확도 문제를 해결하는 고급 경사 하강법

한 번 업데이트할 때마다 전체 데이터를 미분해야 하므로 계산량이 매우 많다.
이러한 점을 보완한 고급 경사 하강법이 등장하면서 딥러닝의 발전 속도는 더 빨라졌다.

+ 확률적 경사 하강법 (Storchastic Gradient, SGD)

전체데이터가 아닌 랜덤한 일부 데이터만을 사용한다. 일부 데이터를 사용하므로 속도가 빨라 자주 업데이트하는 것이 가능하다.

아래 그림을 보면 진폭이 큰 것을 볼 수 있지만 최적의 해에 빠르게 도달한다.

<img width="337" alt="image" src="https://user-images.githubusercontent.com/101916929/215670534-cfb42520-4b2e-4dba-a713-b842faef7afa.png">


***

+ 모멘텀

경사하강법처럼 매번 기울기를 구하지만 이를 통해 오차를 수정하기 전 바로 앞 수정 값과  방향(+,-)을 참고하여 같은 방향으로 일정한 비율만 수정되게 하는 방법이다.

지그재그 형식의 이동을 없애고 이전 이동값을 고려하여 관성효과를 낸다.

<img width="330" alt="image" src="https://user-images.githubusercontent.com/101916929/215670799-3eea590d-2674-4c9c-bf1d-e1ece23c12ba.png">


+ 이외의 고급 경사 하강법

<img width="395" alt="image" src="https://user-images.githubusercontent.com/101916929/215670886-cc35f6c6-3446-4b70-b375-0908ea61cfba.png">
<img width="392" alt="image" src="https://user-images.githubusercontent.com/101916929/215670923-7b816dc6-fe3a-49e7-bfa5-5016ca822cce.png">

워터마크도 보이네요 ㅎㅎ,,

이상 과제 마치겠습니다.

