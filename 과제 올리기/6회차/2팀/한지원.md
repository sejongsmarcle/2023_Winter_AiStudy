# Ch 13. 과적합피하기
## 1. 데이터의 확인과 실행
정확도가 100%인 모델을 얻어도 
앞에서 정리한대로 학습 시킨 데이터를 다시 넣어서 실행을 했기 때문에 
이미 그 데이터에 최적화가 되어있는 모델이라서 결과가 안 좋을 수가 없는 반면 
여기에 다른 데이터를 입력시키면 그 정확도가 떨어지게 되는 것이다.

## 2. 과적합 이해하기
과적합이란 학습 데이터셋 안에서는 일정 수준 이상의 예측 정
확도를 보이지만, 새로운 데이터에 적용하면 잘 맞지 않는 것을 말한다.

## 3. 학습셋과 테스트셋
초기 데이터에서 학습셋과 데이터셋을 구분하는 방법이 있다. 
학습셋으로 딥러닝 모델을 학습시키고 최종적으로 나머지 테스트셋을 입력해서 모델을 테스트 한다면 과적합을 피하는데 도움이 될 것이다.

## 4. 모델저장과 재사용
학습이 끝난 후 테스트해 본 결과가 만족스러울 때 이를 모델로 저장하여 새로운 데이터에 사용할 수 있다.

## 5. k겹교차검증
k겹 교차 검증이란 데이터셋을 여러 개로 나누어 하나씩 테스트셋으로 사용하고 
나머지를 모두 합해서 학습셋으로 사용하는 방법이다.

# Ch 14. 베스트 모델 만들기
## 1. 데이터의 확인과 실행
```python
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import ModelCheckpoint, EarlyStopping
import pandas as pd
import numpy
import tensorflow as tf
import matplotlib.pyplot as plt

# seed 값 설정
seed = 0
numpy.random.seed(seed)
tf.random.set_seed(3)

# 데이터 입력
df_pre = pd.read_csv('../dataset/wine.csv', header=None)
df = df_pre.sample(frac=1)
dataset = df.values
X = dataset[:, 0:12]
Y = dataset[:, 12]

# 모델 설정
model = Sequential()
model. add(Dense(30, input_dim=12, activation = 'relu'))
model.add(Dense(12, activation = 'relu'))
model.add(Dense(8, activation = 'relu'))
model.add(Dense(1, activation = 'sigmoid'))

# 모델 컴파일
model. compile( loss='binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

# 모델 실행
model.fit(X, Y, epochs=200, batch_size=200)

# 결과 출력
print("\n Accuracy: %.4f" % (model.evaluate(X, Y)[1]))
```

## 2. 모델 업데이트하기
모델을 에포크가 진행되면서 모든 값이 저장되는 것이 아니라 테스트 오차를 실행한 
결괏값이 향상되 었을 때만 저장되게 할 수 있다.

## 3. 그래프로 확인하기
앞서 공부한대로 학습셋의 정확도는 시간이 흐를수록 좋아진다. 
하지만 테스트 결과는 어느 정도 이상 시간이 흐르면 
더 나아지지 않는 것을 그래프로 확인할 수 있다.

## 4. 학습의 자동중단
EarlyStopping() 함수를 이용하여 학습이 진행되어도 테스트셋 오차가 줄지 않으면 학습을 멈추게 할 수 있다.
