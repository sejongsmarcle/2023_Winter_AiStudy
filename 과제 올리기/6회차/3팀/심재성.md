https://colab.research.google.com/drive/1uG4JRck0HbfaxNbh7b2FEdJpfOPGNnuR?usp=sharing

***

# 13장 과적합 피하기

+  데이터의 확인과 실행

```python


from keras.models import Sequential
from keras.layers.core import Dense
from sklearn.preprocessing import LabelEncoder

import pandas as pd
import numpy
import tensorflow as tf

# seed 값 설정
numpy.random.seed(3)
tf.random.set_seed(3)

# 데이터 입력
df = pd.read_csv('../dataset/sonar.csv', header=None)
'''
# 데이터 개괄 보기
print(df.info())

# 데이터의 일부분 미리 보기
print(df.head())
'''
dataset = df.values
X = dataset[:,0:60]
Y_obj = dataset[:,60]

# 문자열 변환
e = LabelEncoder()
e.fit(Y_obj)
Y = e.transform(Y_obj)

# 모델 설정
model = Sequential()
model.add(Dense(24,  input_dim=60, activation='relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# 모델 컴파일
model.compile(loss='mean_squared_error',
            optimizer='adam',
            metrics=['accuracy'])

# 모델 실행
model.fit(X, Y, epochs=200, batch_size=5)

# 결과 출력
print("\n Accuracy: %.4f" % (model.evaluate(X, Y)[1]))
```

***

+ 과적합 이해하기

과적합이란 모델이 학습 데이터셋 안에서는 일정 수준 이상의 예측 정확도를 보이지만, 새로운 데이터에 적용하면 잘 맞지 않는 것을 말합니다.

<img width="185" alt="image" src="https://user-images.githubusercontent.com/101916929/218655771-a94e1815-2624-4294-be68-cbd5dce88957.png">

***

+ 학습셋과 테스트셋

과적합을 방지하려면 어떻게 해야 할까.

100개의 데이터가 있다면

70개 정도는 학습셋으로 30개 정도는 테스트셋으로 준비합니다.

머신러닝의 최종 목적은 과거의 데이터를 토대로 새로운 데이터를 예측하는 것입니다.
즉, 새로운 데이터에 사용할 모델을 만드는 것이 최종 목적이므로 테스트셋을 만들어 정확한 평가를 병행해야 한다.

_학습이 깊어져서 학습셋 내부에서의 성공률은 높아져도 테스트셋에서는 효과가 없다면 과적합이 일어나고 있는 것이다._

<img width="200" alt="image" src="https://user-images.githubusercontent.com/101916929/218657081-492d48ca-5e09-4c6c-99e8-918e97aa33c6.png">

***

+ 모델의 저장과 재사용

학습한 결과를 모델로 저장하려면 아래와 같이 실행한다.

```python
from keras.models import load_model

model.save('my_model')
```

이를 불러오려면 다음과 같이 실행한다.

```python

model = load_model('my_model')
```
***
+ k겹 교차 검증

아까처럼 데이터의 70%를 학습셋  30%를 테스트셋으로 사용한다 했을 때
30%만으로는 얼마나 잘 작동하는지 확신하는기에는 쉽지 않다.

그래서 우리는 k겹 교차 검증(k-fold cross validation)을 사용한다. 

k겹 교차검증이란 데이터셋을 여러 개로 나누어 하나씩 테스트셋으로 사용하고 나머지를 모두 합해서 학습셋으로 사용하는 방법이다..

<img width="322" alt="image" src="https://user-images.githubusercontent.com/101916929/218658987-cf9dd20a-ed77-41bb-9a3b-4ee1d32475f0.png">

```python
from keras.models import Sequential
from keras.layers.core import Dense
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold

import numpy
import pandas as pd
import tensorflow as tf

# seed 값 설정
seed = 0
numpy.random.seed(seed)
tf.random.set_seed(3)

df = pd.read_csv('../dataset/sonar.csv', header=None)

dataset = df.values
X = dataset[:,0:60]
Y_obj = dataset[:,60]

e = LabelEncoder()
e.fit(Y_obj)
Y = e.transform(Y_obj)

# 10개의 파일로 쪼갬
n_fold = 10
skf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)

# 빈 accuracy 배열
accuracy = []

# 모델의 설정, 컴파일, 실행
for train, test in skf.split(X, Y):
    model = Sequential()
    model.add(Dense(24, input_dim=60, activation='relu'))
    model.add(Dense(10, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='mean_squared_error',
                  optimizer='adam',
                  metrics=['accuracy'])
    model.fit(X[train], Y[train], epochs=100, batch_size=5)
    k_accuracy = "%.4f" % (model.evaluate(X[test], Y[test])[1])
    accuracy.append(k_accuracy)

# 결과 출력
print("\n %.f fold accuracy:" % n_fold, accuracy)

```

# 14장 베스트 모델 만들기

+ 데이터의 확인과 실행

먼저 df_pre라는 공간에 데이터를 불러옵니다. 그런 다음 sample() 함수를 사용하여 원본 데이터의 몇 %를 사용할지를 지정합니다.

```python

df_pre = pd.read_csv('~~~~.csv', header = None)
df = df_pre.sample(frac=1)

```
frac = 1 은 데이터의 100%를 frac = 0.5 데이터의 50%를 의미

```python
print(df.head(5))
```
<img width="413" alt="image" src="https://user-images.githubusercontent.com/101916929/218660425-0dc90143-279e-4651-b237-00246f306d5a.png">

```python
print(df.info())
```
<img width="338" alt="image" src="https://user-images.githubusercontent.com/101916929/218660541-9aea8eb2-0232-48a5-a36d-30cd03d1122c.png">

***

+ 모델 업데이트하기

모델을 그냥 저장하는 것이 아니라 에포크마다 모델의 정확도를 함께 기록하면서 저장하면
예를 들어, 100번째 에포크를 실행하고 난 결과 오차가 0.0612라면 파일명을 100-0.0612.hdf5가 된다.

```python

from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import ModelCheckpoint

import pandas as pd
import numpy
import os
import tensorflow as tf

# seed 값 설정
seed = 0
numpy.random.seed(3)
tf.random.set_seed(3)

df_pre = pd.read_csv('wine.csv', header=None)
df = df_pre.sample(frac=1)

dataset = df.values
X = dataset[:,0:12]
Y = dataset[:,12]

# 모델 설정
model = Sequential()
model.add(Dense(30, input_dim=12, activation='relu'))
model.add(Dense(12, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# 모델 컴파일
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# 모델 저장 폴더 설정
MODEL_DIR = './model/'  # 모델을 저장하는 폴더
if not os.path.exists(MODEL_DIR):   # 만일 위의 폴더가 존재하지 않으면
  os.mkdir(MODEL_DIR)   # 이 이름의 폴더를 만들어 줌

# 모델 저장 조건 설정
modelpath="./model/{epoch:02d}-{val_loss:.4f}.hdf5"
checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)
  # checkpointer 변수 : 모니터할 값 지정
  # verbose=1 : 해당 함수의 진행 사항 출력(값을 0으로 정하면 출력하지 않음)
  # acc : 학습 정확도, val_acc : 테스트셋 정확도, loss : 학습셋 오차
  # save_best_only=True : 모델이 앞서 저장한 모델보다 나아졌을 때만 저장

# 모델 실행 및 저장
model.fit(X, Y, validation_split=0.2, epochs=200, batch_size=200, verbose=0, callbacks=[checkpointer])
```

***

+ 그래프로 확인하기

데이터를 너무 적게 에포크가 적으면 학습이 안되고 에포크가 너무 높아도 과적합을 일으킨다.

```python
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import ModelCheckpoint

import pandas as pd
import numpy
import os
import matplotlib.pyplot as plt
import tensorflow as tf

# seed 값 설정
numpy.random.seed(3)
tf.random.set_seed(3)

df_pre = pd.read_csv('../dataset/wine.csv', header=None)
df = df_pre.sample(frac=0.15)

dataset = df.values
X = dataset[:,0:12]
Y = dataset[:,12]

# 모델의 설정
model = Sequential()
model.add(Dense(30,  input_dim=12, activation='relu'))
model.add(Dense(12, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# 모델 컴파일
model.compile(loss='binary_crossentropy',
          optimizer='adam',
          metrics=['accuracy'])

# 모델 저장 폴더 설정
MODEL_DIR = './model/'
if not os.path.exists(MODEL_DIR):
   os.mkdir(MODEL_DIR)

# 모델 저장 조건 설정
modelpath="./model/{epoch:02d}-{val_loss:.4f}.hdf5"
checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)

# 모델 실행 및 저장
history = model.fit(X, Y, validation_split=0.33, epochs=3500, batch_size=500)

# y_vloss에 테스트셋으로 실험 결과의 오차 값을 저장
y_vloss=history.history['val_loss']

# y_acc 에 학습 셋으로 측정한 정확도의 값을 저장
y_acc=history.history['accuracy']

# x값을 지정하고 정확도를 파란색으로, 오차를 빨간색으로 표시
x_len = numpy.arange(len(y_acc))
plt.plot(x_len, y_vloss, "o", c="red", markersize=3)
plt.plot(x_len, y_acc, "o", c="blue", markersize=3)

plt.show()

```

<img width="263" alt="image" src="https://user-images.githubusercontent.com/101916929/218663670-cff38fef-7170-4b1d-abb4-a2a075eaaa8a.png">
<img width="185" alt="image" src="https://user-images.githubusercontent.com/101916929/218663719-57726611-29ea-46af-9882-5a91210fe47e.png">
***

+ 학습의 자동중단

학습이 진행될수록 학습셋의 정확도는 올라가지만 과적합 떄문에 테스트셋의 실험결과는 점점 나빠지게 된다.
그래서 학습을 멈추게 하는 함수를 사용하는는데 그것이 EarlyStopping()

```python

from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import EarlyStopping

import pandas as pd
import numpy
import tensorflow as tf

# seed 값 설정
numpy.random.seed(3)
tf.random.set_seed(3)


df_pre = pd.read_csv('../dataset/wine.csv', header=None)
df = df_pre.sample(frac=0.15)

dataset = df.values
X = dataset[:,0:12]
Y = dataset[:,12]

model = Sequential()
model.add(Dense(30,  input_dim=12, activation='relu'))
model.add(Dense(12, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
           optimizer='adam',
           metrics=['accuracy'])

# 자동 중단 설정
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)

# 모델 실행
model.fit(X, Y, validation_split=0.2, epochs=2000, batch_size=500, callbacks=[early_stopping_callback])

# 결과 출력
print("\n Accuracy: %.4f" % (model.evaluate(X, Y)[1]))


```
