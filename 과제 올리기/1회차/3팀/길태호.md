# 3장. 가장 훌륭한 예축선 긋기:선형 회귀
----------

## 3.1 선형회귀의 정의

-선형회귀: 독립변수 x를 사용해 종속변수 y의 움직임을 예측하고 설명하는 작업을 말한다.
-단순선형회귀: x가 하나인 경우
-다중 선형회귀: x가 여러개인 경우

## 3.2 가장 훌륭한 예측선이란?

좌표평면에서 집합 x와 y를 아래와 같이 나타낸 경우, 직선으로 표현될 수 있는 선형의 형태를 띈다는 것을 알 수 있다.   

![image](https://user-images.githubusercontent.com/94752167/211329016-c7032a36-a725-40eb-981d-9c0a4c28f84e.png)
     
이러한 직선은 일차 함수 그래프로 y=ax_b로 표현할 수 있다. 여기서, x는 독립 변수, y는 종속변수이다.    
다라서 x에 따라 y의 값은 달라진다. 또한, 정확한 선을 그리기 위해서는 직선의 기울기 a와 절변b값을 알아야 한다.   
이러한 선형회귀는 정확한 직선을 그리는 작업이다. 이는 최적의 a와 b값을 찾아내는 작업이라고 할 수 있다.     
정확한 직선을 찾으면, 새로운 x에 대한 y값을 찾을 수 있고 이는 머신러닝의 기본 접근 방식과 비슷하다.    

## 3.3 최소 제곱법   

최소 제곱법은 일차함수에서 기울기 a와 y절변 b의 값을 구하는 공식이다. 최소 제곱법은 x가 하나인 단순선형회귀에서만 가능하다. x가 여러개인 다중 선형회귀의 경우에는 경사하강법을 이용한다.           
-기울기 a:    
![image](https://user-images.githubusercontent.com/94752167/211330482-b34de73c-10ae-464e-89da-a985ef37d0ba.png)
     
- y절편 b:    
![image](https://user-images.githubusercontent.com/94752167/211330591-8bed00b0-4880-4f85-9752-4013e76a82e5.png)
     
이를 통해 아래와 같이 예측 선형그래프를 그릴 수 있다.    
![image](https://user-images.githubusercontent.com/94752167/211330849-fc56425d-b69e-46dd-9ee8-5ad56b3c3b8f.png)
     
## 3.4 최소 제곱법 코드
```
import numpy as np
# x 값과 y 값
x=[2, 4 , 6 , 8]
y=[81, 93, 91, 97]
# ><와 7의 평균값
mx = np.mean(x)
my = np.mean(y)
p rin t(" x 의 평균값: " , mx)
p rin t(" y 의 평균값: " , my)
# 기울기 공식의 분모
d iv is o r = sum([ (mx - i)* * 2 fo r i in x ])
# 기울기 공식의 분자
def top(x, mx, y , my):
d = 0
fo r i in rang e(len (x) ) ：
d += ( x [ i] - mx) * ( y [ i ] - my)
return d
dividend = top(x, mx, y , my)
p r in t (" 분 모 :" , d iv is o r)
p r in t (" 분 자 :" , dividend)
# 기울기와 y 절편 구하기
a = dividend / d iv is o r
b = my - (mx*a)
# 출력으로 확인
p r in t("기 울 기 a =", a)
p rin tC 'y 절편 b =", b)
```

https://colab.research.google.com/drive/1P7cVMP4AR5qH66tAqwbMpOy6xJUH6uRZ#scrollTo=sqsR8-YCiJ3c


## 3.5 평균 제곱 오차

최소 제곱법은 한 종류의 입력이 있는 경우에만 사용 가능하다. 여러 개의 입력이 있는 경우에는 임의의 선을 그리고 난 후, 이 선이 얼마나 잘 그려졌는지를 평가하여 조금씩 수정해 가는 방법을 사용한다. 이를 위해 주어진 선의 오차를 평가하는 오차 평가 알고리즘이 필요하다. 이 중 하나인 평균 제곱 오차에 대해 공부해보자.    

## 3.6 잘봇 그은 선 바로잡기    

위에서 언급했듯이, 평균제곱오차는 여러개의 입력이 있는 경우에 사용한다.     
일단 선을 임의로 그린 후 오차를 계산하여 오차가 가장 작은 선을 찾아나가는 것이다.  
아래 그림과 같이 선을 그린 후 빨간색 부분인 오차가 작아지도록 선을 수정해나간다.    
![image](https://user-images.githubusercontent.com/94752167/211333647-cc325e93-a026-4897-b036-855eedd0a907.png)
      
여기서 생각해보아야 할 점은, 각 점에서의 그래프와의 오차를 더하게 되면 제대로 된 오차를 구할 수 없다는 점이다. 아래 두개의 그림을 살펴보자.    

![image](https://user-images.githubusercontent.com/94752167/211333962-2ad1d42f-51bf-4d9d-bdb8-b1d57ad29eab.png)
     
위의 첫번째 그림처럼 모든 점들보다 위에 선이 위치하도록 그릴 경우 오차의 합은 +로 크며, 기울기가 증가할 경우에는 오차가 무한대로 커질 것이다.   
하지만, 두 번쨰 그림처럼 모든 점들보다 아래에 선이 위치하도록 그릴 경우 오차의 합은 -로 크며, 기울기가 감소할 경우에는 오차가 음의 무한대로 커질 것이다.   
이는 구하고자 하는 오차와는 멀다. 따라서, 오차의 부호를 제거하기 위해 오차를 제곱하여 평균적인 오차를 구한다. 이러한 평균제곱오차를 구하는 공식은 아래와 같다.
![image](https://user-images.githubusercontent.com/94752167/211334696-b8c4941c-ebf6-43fd-80fd-9137bf70ebff.png)      

여기서 1/n을 제외한 나머지는 오차의 합이고, n으로 나누어 평균적인 제곱오차를 구한 것이다.    
이를 다시 정리하면 선형 회귀란 임의의 직선을 그어 이에 대한 평균 제곱 오차를 구하고, 이 값을 가장 작게 만들어 주는 a와 b 값을 찾아가는 작업이라고 할 수 있다. 

## 3.7 평균제곱오차 공식 코드
```
import numpy as np
# 기 울 기 a와 y 절편 b
fake_a_b = [3, 76]
# x 가 의 데 이 터 값
data = [[ 2 , 8 1], [4, 93】, [6, 91], [8, 97]]
x = [ i[ 0 ] fo r i in data]
y = [ i [ 1 ] fo r i in data]
# y = ax + b에 a와 b 값을 대입하여 결과를 출력하는 함수
def p re d ic t(x ) ：
re tu rn fake_a_b[〇]*x + fake_a_b[1]
# MSE^ ■수
def mse(y, y_h a t):
re tu rn ( (y-y_hat) ** 2 ) .mean())
# MSE 함수를 각 y 값에 대입하여 최종 값을 구하는 함수
def m se_val(y, p re d ic t_ re s u lt):
re tu rn m se(np.array(y), np. a rra y ( p re d ic t_re s u lt))
# 예측 값이 들어갈 빈 리스트
p re d ic t_re s u lt = []
# 모든 x 값을 한 번씩 대입하여
fo r i in range(len (x) ) ：
# predict_result 리스트를 완성
p re d ic t_ re s u lt. append( p re d ic t( x [ i ] ))
p rint("공 부 한 시 간 =%.f, 실제 점 수 =%.f, 예측 점 수 =%.fn % ( x [ i] , y [ i ] ,
p r e d ic t ( x [ i] ) ) )
# 최종 MSE 출력
printC'mse 최 종 값 : " + s t r (mse_val( p re d ic t_re s u lt, y )) )
```











