
## 인트로

💡 딥러닝의 가장 기본이 되는 두 가지 계산 원리인 **선형 회귀** 와  **로지스틱 회귀**   
그 중 “ **가장 훌륭한 예측선** ”을 찾는  **선형 회귀** 로 시작한당


## 선형 회귀의 정의

💡 머신러닝과 딥러닝은 **‘정보 요소**’에 따라 **‘결과’** 를 예측하는 것! 이를 수학적으로 표현한다면 독립적으로 변하는 **독립 변수 x**, 그에 따라 종속적으로 변하는 **종속 변수 y**로 설명할 수 이따. 즉, 선형 회귀란  **x의 변화에 따라 y의 움직임을 예측하고 설명하는 작업**이다.

***y=ax+b***의 일차 함수의 그래프로 설명 가능! 따라서 **직선의 기울기 a와 y절편 b를 정확히 예측해 내는 과정. 데이터가 없더라도 결과를 유추할 수 있다.**

- 선형 회귀의 종류
    - **하나의** x 값→ **단순 선형 회귀**
    - **여러** 개의 x값→ **다중 선형 회귀**

***y=ax+b***의 일차 함수의 그래프로 설명 가능

## 독립변수가 하나일 때, 최소 제곱법

![image](https://user-images.githubusercontent.com/101866090/211563864-b8a2e3c9-f9a2-4207-b6d5-ee90b2d74d20.png)


→ 이렇게 최소 제곱법을 이용한다면 **오차가 가장 적으면서 주어진 데이터의 특성을 잘 나타내는 직선**을 표현할 수 있다. (**여러 개**의 변수일 때는 **경사 하강법**)

### 최소 제곱법 코드

```python
#변수가 1개일 때의 선형 회귀 구하기

import numpy as np

x=[2,4,6,8]
y=[81,93,91,97]

#**리스트** 원소의 평균을 구해주는 **numpy의 mean함수**
mx=np.mean(x)
my=np.mean(y)

print('x의 평균값:',mx)
print('y의 평균값:',my)

#최소 제곱근의 분모
divisor=sum([(i-mx)**2 for i in x]) **#sum함수와 리스트 컴프리헨션**
print('분모:',divisor)

#최소 제곱근의 분자
def top(x,mx,y,my):  **#사용자 지정 함수**
    d=0
    for i in range(len(x)):
        d+=(x[i]-mx)*(y[i]-my)
    return d

dividend=top(x,mx,y,my)
print('분자:',dividend)

#기울기 구하기
a=dividend/divisor

#y절편 구하기
b=my-(mx*a)

print('기울기:',a)
print('y 절편:',b)
```

- **sum 함수**
    - 인자 :  리스트나 튜플처럼 **인덱스 순환 접근**이 가능하며 **숫자로 이루**어진 것.
    
    [[python] 파이썬 sum함수 정리와 예제](https://blockdmask.tistory.com/413)
    
- **리스트 표현식**
    - 파이썬의 리스트는 **for 반복문과 if 조건문 사용 가능**
    
    ```python
    **#[식 for 변수 in 리스트]**
    #list(식 for변수 in 리스트)
    ```
    
    - 뒤에서 앞으로 읽는 방식으로 해석 가능!
    
    [파이썬 코딩 도장](https://dojang.io/mod/page/view.php?id=2285)
    
    [파이썬 코딩 도장](https://dojang.io/mod/page/view.php?id=2293)
    

## 독립변수가 여러 개일 때, 오차 검증하는 평균 제곱 오차

💡 위에서의 최소 제곱법과 달리 **여러 개의 입력**을 처리해야한다!!

1️⃣ **임의의 선**을 그린다
2️⃣ 선을 잘 그렸는지 평가한다 (**선의 오차 평가**)
3️⃣ 오차가 **최소가 되도록 수정**한다

이번 챕터에서는 독립 변수가 여러 개일 때, 경사 하강법 전에 오차를 계산하는 방법은 **평균 제곱 오차**(**mean square error,MSE**)에 대해 공부한다.

(참고로, 경사 하강법은 나중에 그린 선이 먼저 그린 선보다 더 좋은지 판단하여 오차가 작은 쪽으로 바꾸는 알고리즘이다.)

여기서 말하는 **오차**란, 임의의 선에서 x를 대입했을 때 나오는 ‘예측 값’과 ‘실제 값’의 차이이다. 

$오차= 예측 값 - 실제 값$

**정확한 오차**를 구하기 위해서는 단순한 오차의 합이 아닌 **부호를 없애는 방식**으로 이루어진다.

$오차의 합= \sum (실제 값-예측 값)^2$

**평균 제곱 오차**

$평균 제곱 오차 = \frac{1}{n}\sum_ (실제 값-예측 값)^2$

최종적으로 **선형 회귀**란 임의의 선을 **가정** 하여 **평균 제곱 오차를 구하여, 이 오차를 가장 작게 만들어 주는** a와 b를 찾아가는 작업이다.

이 때 오차를 줄이는 방법은 다음 장인 **‘경사 하강법’** 이다.



### 평균 제곱 오차 공식 코드

```python
#변수가 여러 개일 때
import numpy as np

#가설로 설정한 기울기와 y절편
fake_a_b=[3,76]

#x,y의 실제 데이터
data=[[2,81],[4,93],[6,91],[8,97]]
x=[i[0]for i in data]
y=[i[1]for i in data]

#예측한 y=ax+b의 결과를 출력하는 함수
def predict(x):
    return fake_a_b[0]*x+fake_a_b[1]

#MSE 함수- 평균 제곱 오차
def mse(y,y_hat):
    return (((y-y_hat)**2).mean()) **#배열의 연산과 mean 함수**

#실제 데이터와 예측 데이터를 array로 바꾸어 MSE하는 함수
def mse_val(y,predict_result):
    return mse(np.array(y),np.array(predict_result)) **#리스트를 배열로** 

#예측 값을 모아둔 리스트
predict_result=[]

#데이터와 예측값을 출력
for i in range(len(x)):
    predict_result.append(predict(x[i]))
    print("공부한 시간=%.f,실제 점수=%.f,예측 점수=%.f"%(x[i],y[i],
                                            predict(x[i])))

#평균 제곱 오차
print("mse 최종값:"+str(mse_val(predict_result,y)))
```

- **파이썬에서의 리스트와 배열(넘파이 array)의 차이**
    - **연산**
        - **리스트**에서 덧셈은 원소들을 이어 붙이는 역할을 한다.
        - **array**는 이산 수학에서 배운 행렬 연산과 마찬가지로, **대응하는 원소에 따라 덧셈과 뺄셈을 수행**한다.
    - **메소드**
        - **리스트**에서는 mean을 지원하지 않는다.
        - **배열**의 **평균** 방법 두 가지(**mean이 제공됨.**)
            - np.mean(배열)
            - 배열.mean()

[[Numpy] 파이썬 리스트 vs 넘파이 어레이(배열) 차이](https://jimmy-ai.tistory.com/90)
