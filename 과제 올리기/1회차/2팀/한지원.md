# Ch.3 선형 회귀
## 1. 선형 회귀의 정의
-  선형 회귀란 독립 변수 올 사용해 종속 변수 y의 움직임을 예측하고 설명하는 작업을 말합니다.
-  하나의 x 값만으로도 y 값을 설명할 수 있을 때 이를 단순 선형 회귀(simple linear regression)라고 합니다.
-  x 값이 여러 개 필요할 때는 다중 선형 회귀(multiple linear regression)라고 합니다.

## 2. 가장 훌륭한 예측선이란?
- 좌표 평면에 놓여진 점들의 특징을 가장 잘 나타내는 직선입니다.   
- y = ax + b 와 같은 식으로 일차함수를 표현하고   
- 선형 회귀는 결국 최적의 a 값과 b 값을 찾아내는 작업이라 할 수 있습니다.

## 3. 최소 제곱법
- 최소 제곱법(mediod of least squares)을 사용하면 일차 함수의 기울기 a와 절편 b를 바로 구할 수 있습니다.   
- 최소 제곱법은 주어진 x 의 값이 하나일 때 적용이 가능합니다.   
- 최소 제곱법을 이용해 기울기 a를 구하는 방법은 다음과 같습니다.
![image](https://user-images.githubusercontent.com/101804119/211442755-7052c6f6-c9c4-436f-999b-4394fcc9a600.png)
- 이는 x의 편차를 제곱해서 합한 값을 분모, x와 y의 편차를 곱해서 합한 값을 분자로 놓는 것과 같습니다.
- 최소 제곱법을 이용해 절편 b를 구하는 방법은 다음과 같습니다.
![image](https://user-images.githubusercontent.com/101804119/211438915-caed2c78-6fec-4903-a770-a8474e82fe0b.png)
- y의 평균에서 x의 평균과 기울기의 곱을 빼면 6의 값이 나온다는 의미입니다.

## 4. 코딩으로 확인하는 최소 제곱

```python
import numpy as np
# x 값과 y 값
x=[2, 4, 6, 8]
y=[81, 93, 91, 97]

# x와 y의 평균값
# x 모든 원소의 평균을 구하는 넘파이 함수는 mean( )
mx = np.mean(x)
my = np.mean(y)
print("x의 평균값:", mx)
print("y의 평균값:", my)

# 기울기 공식의 분모
divisor = sum([(mx - i)**2 for i in x])

# 기울기 공식의 분자
def top (x, mx, y, my):
    d = 0
    for i in range (len(x)):
        d += (x[i] - mx) * (y[i] - my)
    return d
dividend = top(x, mx, y, my)

print("분모:", divisor)
print("분자:", dividend)

# 기울기와 y 절편 구하기
a = dividend / divisor
b = my - (mx*a)

# 출력으로 확인
print("기울기 a =", a)
print("y 절편 b =", b)
```

``` python
x의 평균값: 5.0
y의 평균값: 90.5
분모: 20.0
분자: 46.0
기울기 a = 2.3
y 절편 b = 79.0
```

## 5. 평균 제곱 오차
- 딥러닝은 대부분 입력 값이 여러 개인 상황에서 쓰이기 때문에 복잡한 연산 과정이 뒤따릅니다.
- 여러 개의 입력 값을 계산할 때는 임의의 선을 그리고 난 후, 이 선이 얼마나 잘 그려졌는지를 평가하여 조금씩 수정해 가는 방법을 사용합니다.

## 6. 잘못 그은 선 바로잡기
- 오차의 합에 대한 식은 다음과 같습니다.   
![image](https://user-images.githubusercontent.com/101804119/211441067-c5559130-f274-41fb-a92a-29b1dedc761d.png)
- 여기서 i는 x가 나오는 순서, n은 x원소의 총 개수를 의미합니다.
- yₗ는 xₗ에 대응하는 '실제 값'입니다.
- yₗ는 xₗ가 대입되었을 때 직선의 방정식이 만드는 '예측 값'입니다.   

- 위에서 구한 값을 n으로 나누면 오차 합의 평균을 구할 수 있습니다. 이를 평균 제곱 오차(Mean SquaredError. MSE)라고 부릅니다.
![image](https://user-images.githubusercontent.com/101804119/211441503-9013caee-035a-4a25-b149-71f89a137ed0.png)

- 선형 회귀란 임의의 직선을 그어 이에 대한 평균 제곱 오차를 구하고, 이 값을 가장 작게 만들어 주는 a와 b 값을 찾아가는 작업입니다.

## 7. 코딩으로 확인하는 평균 제곱 오차

``` python
import numpy as np
# 기울기 a와 y 절편 b
fake_a_b = [3, 76]

# x, y의 데이터 값
data = [[2, 81], [4, 93], [6, 91], [8, 97]]
x = [i[0] for i in data]
y = [i[1] for i in data]

# y = ax + b에 a와 b 값을 대입하여 결과를 출력하는 함수
def predict(x):
    return fake_a_b[0]*x + fake_a_b[1]

# MSE 함수
def mse(y, y_hat):
    return ((y-y_hat) ** 2 ).mean()

# MSE 함수를 각 y 값에 대입하여 최종 값을 구하는 함수
def mse_val(y, predict_result):
    return mse(np.array(y), np.array(predict_result))
    
# 예측 값이 들어갈 빈 리스트
predict_result = []

# 모든 x 값을 한 번씩 대입하여
for i in range(len(x)):
    # predict_result 리스트를 완성
    predict_result.append(predict(x[i]))
    print("공부한 시간=%.f, 실제 점수=%.f, 예측 점수=%.f" % (x[i], y[i], predict(x[i])))

# 최종 MSE 출력
print("mse 최종값: " + str(mse_val(predict_result, y)))
```

``` python
공부한 시간=2, 실제 점수=81, 예측 점수=82
공부한 시간=4, 실제 점수=93, 예측 점수=88
공부한 시간=6, 실제 점수=91, 예측 점수=94
공부한 시간=8, 실제 점수=97, 예측 점수=100
mse 최종값: 11.0
```
