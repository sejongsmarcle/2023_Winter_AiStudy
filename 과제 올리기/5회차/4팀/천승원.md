# 10장. tensorflow를 통해 모델 설계하기
### Sequential()
Sequential() 함수를 model로 선언해놓고 model.add()라는 라인을 통해 층을 쌓아나가는 방식. 첫 번째 층은 입력층과 은닉층의 역할을 겸하고(Dense함수의 input_dim) 맨 마지막 층은 결과를 출력하는 출력층이 되며, 나머지 층은 은닉층의 역할을 한다.

### Dense()
각각의 층은 Dense(만들 노드의 개수, input_dim=입력데이터에서 가져올 개수, activation=활성화함수)라는 함수로 그 구조를 구체화한다. 참고로 마지막 출력층의 경우, 출력값을 하나로 정해서 보여줘야 하므로 출력층의 노드수는 1개이다.

### model.complie()
model.complie(loss=‘오차함수(ex-평균제곱오차)’, optimizer=‘최적화방법(ex-경사하강법)’, mertrics=[‘모델수행결과 설정’])함수는 앞서 선언한 모델이 효과적으로 구현될 수 있게 여러 가지 환경을 설정해 주면서 컴파일하는 부분이다.

### 데이터의 구분
정확한 구분별 정의를 알려주지는 않았으나, 샘플 = 각 데이터를 구분하는 명칭이나 번호(ex-환자의  이름 or 환자번호), 속성 = 각 데이터가 가진 정보(ex-수술이력, 나이, 건강상태 등등), 클래스 = 구할 데이터 y(ex-생존여부)를 의미하는 것으로 볼 수 있을 것 같다.

### model.fit()
앞서 컴파일 단계에서 정해진 환경을 주어진 데이터를 불러와 실행시킬 때 사용하는 함수. model.fit(X, Y, epochs=데이터를 반복(재사용)할 횟수, batch_size=몇개씩 끊어서 집어 넣을것인지) -> batch_size의 경우 너무 크면 학습속도가 느려지고, 너무 작으면 각 실행값의 편차가 생겨 전체 결과값이 불안정해질 수 있다. 

# 실습 코드
++) colab에서 데이터 업로드 하는 방법(?)- 확실하진 않음.      
옆의 파일 아이콘 누르고 sample_data파일에 자신이 사용할 csv 데이터 파일을 업로드. /content/sample_data/를 앞에 붙여서 경로 지정(/content가 현재 위치, sample_data내부이므로 경로 지정)

![image](https://user-images.githubusercontent.com/67413252/217156209-00263a6f-985d-4df6-94f1-cd8e19cc5c26.png)

++) csv파일 두 번 클릭하면 내부에 어떤식으로 되어있는지 간략하게 확인 가능.       

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import numpy as np
import tensorflow as tf
#반복 실행시 같은 결과값 출력 위함 - 시드 지정
np.random.seed(3)
tf.random.set_seed(3)
Data_set = np.loadtxt("/content/sample_data/ThoraricSurgery.csv", delimiter=",")
#0~16행까지 X, 결과값인 Y는 17행
X = Data_set[:, 0:17]
Y = Data_set[:, 17]
model = Sequential()
model.add(Dense(30, input_dim=17, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
model.fit(X, Y, epochs=100, batch_size=10)
```
![image](https://user-images.githubusercontent.com/67413252/217156274-35b0e02c-f40c-4a85-bf29-4cfa107ed0de.png)


# 교차 엔트로피
어떤 사건이 발생할 확률이 클수록 우리는 그 사건이 발생해도 대수롭지 않게 여기게 되는데, 그것을 우리는 정보량이 작다고 한다. 반대로 어떤 사건이 발생할 확률이 작을수록 그 사건을 놀랍게 보게 되는데, 그것을 우리는 정보량이 크다고 한다. 엔트로피는 이러한 정보량의 기댓값, 즉 어떤 상태에서의 불확실성으로 평균 정보량을 의미한다. 따라서 확률이 낮을수록 엔트로피는 커지고, 확률이 높을수록 엔트로피는 작아진다. 
이러한 엔트로피를 로그 취하고 더하면 교차 엔트로피 공식이 발생한다.
https://gooopy.tistory.com/63

# 원-핫 인코딩
이 개념은 데이터를 수많은 0과 한 개의 1의 값으로 데이터를 구별하는 인코딩이다. 텍스트형태의 범주형 데이터를 머신러닝 프로젝트에서 활용하기 위해서는 이 데이터를 컴퓨터가 이해할 수 있도록 숫자 데이터로 바꿔주어야 하는데, 일반적인 방식으로는 데이터 연속성 문제가 생길 수 있다. (컴퓨터=1, 핸드폰=3, 노트북=5로 분류할 때, 핸드폰을 나타내는 숫자데이터가 3이라고 해서 컴퓨터와 노트북의 중간을 의미하는 것이 아니다.)
그리고 애초에 활성화함수를 적용하기 위해서는 Y값이 숫자 0과 1로 이루어져 있어야 하기도 하다.(교재에도 나와있음.)
따라서 1 => [1, 0, 0, 0, 0], 3 => [0, 0, 1, 0, 0], 4 -> [0, 0, 0, 1, 0] 이런식으로 바꿔주면 각 1의 위치에 따라 범주를 확실하게 구분할 수 있게 된다. 이것이 바로 원-핫 인코딩의 개념이다.
https://needjarvis.tistory.com/565
https://blog.naver.com/PostView.naver?blogId=baek2sm&logNo=221802541443&parentCategoryNo=&categoryNo=75&viewDate=&isShowPopularPosts=false&from=postView

# 11장. 데이터 다루기
### 데이터 확인하는 부분
```python
import numpy as np
import pandas as pd
#데이터 분석 스터디에서 배웠던 내용과 중복되는 내용
# read_csv로 csv파일 불러오고 색인 넣는 것. head()함수로 앞의 줄 확인하기.
# info()로 데이터 프레임의 정보 확인, describe()로 샘플수, 평균, 표준편차 등등 확인하기.
# [‘ ’, ‘ ’]로 원하는 색인의 정보 뽑아내는 법(대괄호 갯수는 주의할것. 아마 슬라이스 할때는 1개, 각각 뽑아낼 때는 2개였던것으로 기억함). 137p부터 140p까지.
df = pd.read_csv("/content/sample_data/pima-indians-diabetes.csv", names=["pregnant", "plasma", "pressure", "thickness", "insulin", "BMI", "predigree", "age", "class"])
print(df.head(5))
print(df.info())

print(df[['pregnant', 'class']].groupby(['pregnant'], as_index=False).mean().sort_values(by='pregnant', ascending=False))
# groupby – 어떤 정보를 기준으로 새 그룹을 만들어줌, as_index=그 정보 옆에 새로운 인덱스 넣어주는 옵션.
# mean()을 걸어주면 그 횟수들의 평균을 구해줄 수 있다.
# sort_values()는 값의 정렬- 무엇을 기준으로 할지, 그리고 오름차순할지 내림차순할지 결정한다.
```
![image](https://user-images.githubusercontent.com/67413252/217156412-5b21f9c1-cb42-4512-85fe-a6ffb43c2438.png)

### 데이터 시각화하는 부분
```python
import matplotlib.pyplot as plt
# 3주차 실습문제에서 했던 내용.
import seaborn as sns
plt.figure(figsize=(8,8))
sns.heatmap(df.corr(), linewidths=0.1, vmax=0.5, cmap=plt.cm.gist_heat, linecolor='white', annot=True)
# class열에서 plasma가 0.47로 가장 밝으므로, 가장 연관이 깊다고 볼 수 있다.

# 행과 열 방향으로 서로 다른 정보를 나타내는 서브플롯을 만들 수 있도록 하는 함수. https://steadiness-193.tistory.com/201
# 기준으로 구분할 색인을 정함
grid = sns.FacetGrid(df, col='class')
# 그래프의 종류와 종류에 맞는 컬럼, 범위 전달.(히스토그램, plasma)
grid.map(plt.hist, 'plasma', bins=10)
plt.show()
```
![image](https://user-images.githubusercontent.com/67413252/217156471-0877e88a-d203-4e17-8bed-27503212e7b5.png)

### 이를 바탕으로 실제로 피마 인디언의 당뇨병을 예측하는 코드
```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
#일정한값 내기 위해 seed값 설정
np.random.seed(3)
tf.random.set_seed(3)
#데이터 셋 구분
dataset = np.loadtxt("/content/sample_data/pima-indians-diabetes.csv", delimiter=",")
X = dataset[:,0:8]
Y = dataset[:,8]
#모델 생성, 신경망의 노드 수랑 신경망 층수, 활성화 함수를 왜 이렇게 정하는건지는 확실하게 모르겠음.
model = Sequential()
model.add(Dense(12, input_dim=8, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, Y, epochs=200, batch_size=10)
print("\n Accuracy: %.4f" %(model.evaluate(X, Y)[1]))
```
![image](https://user-images.githubusercontent.com/67413252/217156519-3c169454-756f-4149-94f1-5a836ed11e3f.png)

# 12장 다중분류문제 해결하기
### 데이터를 확인하고 시각화하는 부분
```python
import pandas as pd
df = pd.read_csv("/content/sample_data/Iris.csv", names=["sepal_length", "sepal_width", "petal_length", "petal_width", "species"])
print(df.head())
print(df.info())
import matplotlib.pyplot as plt
import seaborn as sns
#pairplot - 두개씩 묶을수 있는 모든 쌍(pair)에 대하여 그래프를 그림(plot) https://steadiness-193.tistory.com/198
#같은 변수끼리 짝을 이루는 대각선 방향으로는 그래프를, 나머지는 산점도를 그린다. hue옵션은 나누는 기준을 추가하는 것(species에 따라 색깔이 다름)
#꽃잎과 꽃받침의 크기와 너비가 품종별로 차이가 있음을 확인할 수 있음
sns.pairplot(df, hue='species');
plt.show()
```
![image](https://user-images.githubusercontent.com/67413252/217156613-6d02ecf1-5063-4ee4-bf6a-9e857c8e6163.png)

### 실제로 아이리스 품종을 분류하는 코드
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import pandas as pd
import numpy as np
import tensorflow as tf
np.random.seed(3)
tf.random.set_seed(3)
df = pd.read_csv("/content/sample_data/Iris.csv", names=["sepal_length", "sepal_width", "petal_length", "petal_width", "species"])
dataset = df.values #값만 따로 추출함.
X = dataset[:, 0:4].astype(float) #float로 dtype를 변경하여 X에 저장, 데이터 프레임의 경우 float64로 저장하기 때문에 이 코드를 사용한듯.
Y_obj = dataset[:,4]
#y값이 숫자가 아니라 문자열이므로, 문자열을 숫자로 바꿔 나타내줘야함. setosa를 1로, versicolor를 2로 이런식으로 각 범주를 숫자로 치환 
#라벨 인코더 https://steadiness-193.tistory.com/243
from sklearn.preprocessing import LabelEncoder
e = LabelEncoder()
e.fit(Y_obj)
Y = e.transform(Y_obj)
#각 범주를 나타내는 숫자들을 0과 1의 형태로 바꿔줌. 원-핫 인코딩. 
from tensorflow.python.keras.utils import np_utils #이거 교재 코드 그대로 하면 실행 안됨. cannot import name 'np_utils' from 'tensorflow.keras.utils'에러가 뜨므로 반드시 tensorflow.python.keras.utils로 import할것.
Y_encoded = tf.keras.utils.to_categorical(Y)
#소프트멕스를 활용하여 신경망 구성
#소프트 멕스 - 다중 분류 문제에서 주로 사용되며, 1로 값을 정규화하는 함수 https://heeya-stupidbutstudying.tistory.com/entry/ML-%ED%99%9C%EC%84%B1%ED%99%94-%ED%95%A8%EC%88%98Activation-Function
model = Sequential()
model.add(Dense(16, input_dim=4, activation='relu'))
model.add(Dense(3, activation='softmax'))
#신경망 컴파일 후 학습, 후 결과 출력
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, Y_encoded, epochs=50, batch_size=1)
print("\n Accuracy: %.4f" %(model.evaluate(X, Y_encoded)[1]))
```
![image](https://user-images.githubusercontent.com/67413252/217156656-232ec027-caa6-4b6a-bbca-fc782a3d8cff.png)

