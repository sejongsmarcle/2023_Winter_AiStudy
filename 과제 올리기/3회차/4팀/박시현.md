## 실습 문제

### 코드 - 직접 수식

```python
import numpy as np
import matplotlib.pyplot as plt

data=[[3,6,70],[6,4,75],[9,5,80],[12,2,90]]

x1=[i[0] for i in data]
x2=[i[1] for i in data]
y=[i[2] for i in data]

import numpy as np
import matplotlib.pyplot as plt

data=[[3,6,70],[6,4,75],[9,5,80],[12,2,90]]

x1=[i[0] for i in data]
x2=[i[1] for i in data]
y=[i[2] for i in data]

#경사 하강법

#기울기와 y절편의 초기값 설정
a1=0
a2=0
b=0

#계산하기 위해 배열로 바꾸기
x1_data=np.array(x1)
x2_data=np.array(x2)
y_data=np.array(y)

#학습률과 에코프 설정
lr=0.01
epochs=2000

#기울기와 y절편 찾기 시작
for i in range(epochs):
    y_pred=a1*x1_data+a2*x2_data+b
    error=y_pred-y_data
    
    #기울기와 y절편의 기울기
    a1_diff=(2/len(x1_data))*sum(x1_data*(error))
    a2_diff=(2/len(x1_data))*sum(x2_data*(error))
    b_diff=(2/len(x1_data))*sum(error)
    
    #기울기와 y절편의 수정과정
    a1=a1-lr*a1_diff
    a2=a2-lr*a2_diff
    b=b-lr*b_diff
    

result=a1*8+a2*4+b

print('8시간을 공부하고 4시간의 과외를 받을 경우, 예상 점수는 %.2f점입니다'%(result))

# 3D 그래프 만들기
#그래프 환경 설정
ax=plt.axes(projection='3d')
ax.set_xlabel('study_hours')
ax.set_ylabel('private_class')
ax.set_zlabel('score')
#y축과 z축의 간격 설정
ax.set_yticks(np.arange(2,6.5,step=0.5))
ax.set_zticks(np.arange(70,92.5,step=2.5))
#데이터 산점도로 표현하기
ax.scatter(x1,x2,y)
#그래프 보여주기
plt.show()
```

<aside>
💡 처음에 학습률을 0.02로 설정하였더니 기울기와 y절편이 nan으로 출력되었다. 이는 학습률이 너무 큰 경우 기울기가 점차 커지더니 비정상적으로 커져 발산하는 기울기 폭주의 문제가 발생하였다.

</aside>

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2cd838d8-ac61-49b8-9deb-aa5426a3be0a/Untitled.png)

[[ Deep Learning 02 ] 경사하강 학습법 ( Gradient Descent ) , 옵티 마이저 ( Optimizer )](https://hazel01.tistory.com/36)

### 코드- tensorflow 이용

```python
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

data=[[3,6,70],[6,4,75],[9,5,80],[12,2,90]]

x1=np.array([i[0] for i in data])
x2=np.array([i[1] for i in data])
x=np.array([[i[0],i[1]] for i in data])
y=np.array([i[2] for i in data])

model=Sequential()
model.add(Dense(1,input_dim=2,activation='linear'))
model.compile(optimizer='sgd',loss='mse')
model.fit(x,y,epochs=2000)

#임의의 시간을 넣어 점수를 예측하는 모델 테스트
hour= 8
private_class=4
prediction=model.predict([[hour,private_class]])
print('8시간을 공부하고 4시간의 과외를 받을 경우, 예상 점수는 %.2f점입니다'%(prediction))

# 3D 그래프 만들기
#그래프 환경 설정
ax=plt.axes(projection='3d')
ax.set_xlabel('study_hours')
ax.set_ylabel('private_class')
ax.set_zlabel('score')
#y축과 z축의 간격 설정
ax.set_yticks(np.arange(2,6.5,step=0.5))
ax.set_zticks(np.arange(70,92.5,step=2.5))
#데이터 산점도로 표현하기
ax.scatter(x1,x2,y)
#그래프 보여주기
plt.show()
```
## 참 거짓 판단 장치: 로지스틱 회귀

<aside>
💡 딥러닝의 가장 기본이 되는 두 가지 계산 원리인 **선형 회귀**와 **로지스틱 회귀**
그 중 로지스틱 회귀에 대한 단원이다. 

✅ **로지스틱 회귀**
참과 거짓을 판단하는 과정

✅ **딥러닝의 작동 원리**
**딥러닝**을 수행하는 것은 전달받은 정보를 놓고 **참과 거짓을 판단**하여 복잡한 연산을 한 끝에 최적의 예측 값을 내놓는 작업이다.  즉, **로지스틱 회귀**를 이용하여 T/F 판단 장치를 만들어 **입력값의 특징을 추출**하고 모델에 저장하여 비슷한 질문이 나왔을 때 **저장해놨던 모델**을 꺼내 답하는 원리이다.

</aside>

## 로지스틱 회귀의 정의

<aside>
💡 입력값과 출력값의 관계가 직선의 형태로 해결되는 선형 회귀와는 달리 **직선으로 표현되지 않는 관계도 존재.** 예를 들어, ****출력값이 True(1), False(0)로 구분되는 경우 점들의 특성을 나타내기 위해 **S자 형태**로 그래프를 나타내야 함.

</aside>

### 기본 개념-시그모이드 함수

<aside>
💡 **밑의 값이 자연 상수 e**인 함수로 다음과 같은 식으로 나타낼 수 있음

$f(x)={1 \over 1+e^-x}$ 

x가 **커질수록 1에** 가까워지고 **작아질수록 0**에 가까워짐. **두 개의 값** 중 하나를 고를 때 유용하게 쓰이는 함수이다.

</aside>

<aside>
💡 **시그모이드** 함수→ **로지스틱 회귀** 

$y={1 \over 1+e^-(ax+b)}$ 

**경사도 a**
a가 커지면 경사가 커지고, a가 작아지면 경사가 작아진다

**그래프의 좌우 이동 b**
b가 크고 작아짐에 따라 그래프가 이동함

</aside>

<aside>
💡 **경사도 a**

a가 **커질** 때 경사가 **커지고** a가 작아지면 경사가 작아진다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5afa364a-e8a1-4cf3-a171-868f6821f071/Untitled.png)   
  
a 값이 작을 때 오차가 무한대로 커지며, a 값이 커진다고 해서 오차가 점점 작아진다. 이렇게 a에 따라 **오차 그래프**를 만들 수 있다.  

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/975eacef-4f65-4241-99df-d48619b6c162/Untitled.png)

</aside>

<aside>
💡 **그래프의 좌우 이동 b**

b의 크기에 따라 그래프가 다음과 같이 이동한다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3fd0cbcb-0219-4f23-8fef-562d40ce3541/Untitled.png)

b가 너무 크거나 작을 경우에 오차 그래프는 이차 함수 그래프와 유사한 형태로 나타난다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a7f230e0-f3dc-41b4-a749-422aac05f8b8/Untitled.png)

</aside>

## 오차 공식 - 로그 함수

<aside>
💡 결국 중요한 것은 a와 b의 값을 구하는 것이네요 ! 경사 하강법을 이용하여 오차를 구한 뒤 작은 쪽으로 이동시켜 봅시다 ! 하지만 요건 직선 그래프가 아닌 시그모이드 함수 그래프죠? 이 그래프의 특징을 이용하여 오차 공식을 도출해 봅시다😊 

실제값이 0일 때 1과 가까워지면 오차가 커지고 실제값이 1일 때 0과 가까워지면 오차가 커지므로 이는 **로그 함수**와 연결지을 수 있어요!

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5c183f18-174c-4f71-ae63-13032538ccdd/Untitled.png)

$-(y_data*logh+(1-y_data)*log(1-h))$

**로지스틱 회귀의** **오차를 구하는 함수**

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0bf6bb9a-2d22-47b8-ab9b-351558e417cd/Untitled.png)

</aside>

## 로지스틱 회귀 코딩하기

### 코드

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#공부 시간 x와 합격 여부 y의 리스트 만들기
data=[[2,0],[4,0],[6,0],[8,1],[10,1],[12,1],[14,1]]

x_data=[i[0] for i in data]
y_data=[i[1] for i in data]

#그래프로 나타내기
plt.scatter(x_data,y_data)
plt.xlim(0,15)
plt.ylim(-.1,1.1)

#기울기와 y절편 값 초기화
a=0
b=0

#학습률
lr=0.05

#시그모이드 함수 정의
def sigmoid(x):
    return 1/(1+np.e**(-x))

#현재의 a 값과 b 값 출력
for i in range(2001):
    for x_data,y_data in data:
        a_diff=x_data*(sigmoid(a*x_data+b)-y_data) **#편미분 과정은 생략한다.**
        b_diff=sigmoid(a*x_data+b)-y_data
        a=a-lr*a_diff
        b=b-lr*b_diff
        
        if i%1000==0:
            print("epoch=%.f, 기울기=%0.4f, 절편=%0.4f"%(i,a,b))

#앞서 구한 기울기와 절편으로 시그모이드 그래프 그리기
plt.scatter(x_data,y_data)
plt.xlim(0,15)
plt.ylim(-.1,1.1)
x_range=(np.arange(0,15,0.1))
plt.plot(x_range,np.array([sigmoid(a*x+b) for x in x_range]))
plt.show()
```

<aside>
💡 **입력 값이 세 개** 이상의 입력 값을 다룬다면 시그모이드 함수가 아닌 **소프트맥스**라는 함수를 써야 한다!

</aside>

## 로지스틱 to 퍼셉트론

<aside>
💡 $y=a_1x_1+a_2x_2+b$

이는 x1,x2의 입력 값이 **가중치** a1,a2와 만나고 **bias** b를 더해 **시그모이드 함수**를 거쳐 1 또는 0의 출력 값을 출력한다. 이러한 **퍼셉트론 개념**은 어떻게 딥러닝의 골격인 신경망을 구성할까? 다음 차시 ㄱㄱ

</aside>

-----
-----

## 사진이 안 보인다면 노션을 확인해주세요!
[박시현 notion](https://spotless-fireman-cc9.notion.site/80766fd3d588459ebd0ef5759b7bb97d)
