## 참 거짓 판단 장치: 로지스틱 회귀

<aside>
💡 딥러닝의 가장 기본이 되는 두 가지 계산 원리인 **선형 회귀**와 **로지스틱 회귀**
그 중 로지스틱 회귀에 대한 단원이다. 

✅ **로지스틱 회귀**
참과 거짓을 판단하는 과정

✅ **딥러닝의 작동 원리**
**딥러닝**을 수행하는 것은 전달받은 정보를 놓고 **참과 거짓을 판단**하여 복잡한 연산을 한 끝에 최적의 예측 값을 내놓는 작업이다.  즉, **로지스틱 회귀**를 이용하여 T/F 판단 장치를 만들어 **입력값의 특징을 추출**하고 모델에 저장하여 비슷한 질문이 나왔을 때 **저장해놨던 모델**을 꺼내 답하는 원리이다.

</aside>

## 로지스틱 회귀의 정의

<aside>
💡 입력값과 출력값의 관계가 직선의 형태로 해결되는 선형 회귀와는 달리 **직선으로 표현되지 않는 관계도 존재.** 예를 들어, ****출력값이 True(1), False(0)로 구분되는 경우 점들의 특성을 나타내기 위해 **S자 형태**로 그래프를 나타내야 함.

</aside>

### 기본 개념-시그모이드 함수

<aside>
💡 **밑의 값이 자연 상수 e**인 함수로 다음과 같은 식으로 나타낼 수 있음

$f(x)={1 \over 1+e^-x}$ 

x가 **커질수록 1에** 가까워지고 **작아질수록 0**에 가까워짐. **두 개의 값** 중 하나를 고를 때 유용하게 쓰이는 함수이다.

</aside>

<aside>
💡 **시그모이드** 함수→ **로지스틱 회귀** 

$y={1 \over 1+e^-(ax+b)}$ 

**경사도 a**
a가 커지면 경사가 커지고, a가 작아지면 경사가 작아진다

**그래프의 좌우 이동 b**
b가 크고 작아짐에 따라 그래프가 이동함

</aside>

<aside>
💡 **경사도 a**

a가 **커질** 때 경사가 **커지고** a가 작아지면 경사가 작아진다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5afa364a-e8a1-4cf3-a171-868f6821f071/Untitled.png)   
  
a 값이 작을 때 오차가 무한대로 커지며, a 값이 커진다고 해서 오차가 점점 작아진다. 이렇게 a에 따라 **오차 그래프**를 만들 수 있다.  

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/975eacef-4f65-4241-99df-d48619b6c162/Untitled.png)

</aside>

<aside>
💡 **그래프의 좌우 이동 b**

b의 크기에 따라 그래프가 다음과 같이 이동한다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3fd0cbcb-0219-4f23-8fef-562d40ce3541/Untitled.png)

b가 너무 크거나 작을 경우에 오차 그래프는 이차 함수 그래프와 유사한 형태로 나타난다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a7f230e0-f3dc-41b4-a749-422aac05f8b8/Untitled.png)

</aside>

## 오차 공식 - 로그 함수

<aside>
💡 결국 중요한 것은 a와 b의 값을 구하는 것이네요 ! 경사 하강법을 이용하여 오차를 구한 뒤 작은 쪽으로 이동시켜 봅시다 ! 하지만 요건 직선 그래프가 아닌 시그모이드 함수 그래프죠? 이 그래프의 특징을 이용하여 오차 공식을 도출해 봅시다😊 

실제값이 0일 때 1과 가까워지면 오차가 커지고 실제값이 1일 때 0과 가까워지면 오차가 커지므로 이는 **로그 함수**와 연결지을 수 있어요!

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5c183f18-174c-4f71-ae63-13032538ccdd/Untitled.png)

$-(y_data*logh+(1-y_data)*log(1-h))$

**로지스틱 회귀의** **오차를 구하는 함수**

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0bf6bb9a-2d22-47b8-ab9b-351558e417cd/Untitled.png)

</aside>

## 로지스틱 회귀 코딩하기

### 코드

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#공부 시간 x와 합격 여부 y의 리스트 만들기
data=[[2,0],[4,0],[6,0],[8,1],[10,1],[12,1],[14,1]]

x_data=[i[0] for i in data]
y_data=[i[1] for i in data]

#그래프로 나타내기
plt.scatter(x_data,y_data)
plt.xlim(0,15)
plt.ylim(-.1,1.1)

#기울기와 y절편 값 초기화
a=0
b=0

#학습률
lr=0.05

#시그모이드 함수 정의
def sigmoid(x):
    return 1/(1+np.e**(-x))

#현재의 a 값과 b 값 출력
for i in range(2001):
    for x_data,y_data in data:
        a_diff=x_data*(sigmoid(a*x_data+b)-y_data) **#편미분 과정은 생략한다.**
        b_diff=sigmoid(a*x_data+b)-y_data
        a=a-lr*a_diff
        b=b-lr*b_diff
        
        if i%1000==0:
            print("epoch=%.f, 기울기=%0.4f, 절편=%0.4f"%(i,a,b))

#앞서 구한 기울기와 절편으로 시그모이드 그래프 그리기
plt.scatter(x_data,y_data)
plt.xlim(0,15)
plt.ylim(-.1,1.1)
x_range=(np.arange(0,15,0.1))
plt.plot(x_range,np.array([sigmoid(a*x+b) for x in x_range]))
plt.show()
```

<aside>
💡 **입력 값이 세 개** 이상의 입력 값을 다룬다면 시그모이드 함수가 아닌 **소프트맥스**라는 함수를 써야 한다!

</aside>

## 로지스틱 to 퍼셉트론

<aside>
💡 $y=a_1x_1+a_2x_2+b$

이는 x1,x2의 입력 값이 **가중치** a1,a2와 만나고 **bias** b를 더해 **시그모이드 함수**를 거쳐 1 또는 0의 출력 값을 출력한다. 이러한 **퍼셉트론 개념**은 어떻게 딥러닝의 골격인 신경망을 구성할까? 다음 차시 ㄱㄱ

</aside>

-----
-----

## 사진이 안 보인다면 노션을 확인해주세요!
[박시현 notion](https://spotless-fireman-cc9.notion.site/80766fd3d588459ebd0ef5759b7bb97d)
