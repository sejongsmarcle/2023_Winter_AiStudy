# 4장 실습문제 풀이
-----------
https://colab.research.google.com/drive/1AUFJ5TsCLl0u27hWZYjLcax6daf5TSnn#scrollTo=VIyBcSFshT-y

+추가적으로 교재 4장의 코드를 활용하여 아래와 같이 구현할 수도 있다.
https://colab.research.google.com/drive/1FuhpdLN7gxxF143EH7NhVAr9wFqXuvdE#scrollTo=Va44lHd3xxc5

# 5장: 참 거짓 판단 장치: 로지스틱 회귀
------------
로지스틱 회귀란 참과 거짓 중에 하나를 내놓는 과정이다. 참인지 거짓인지를 구분하는 로지스틱 회귀의 원리를 이용해 '참, 거짓 미니 판단 장치'를 만들어 주어진 입력 값의 특징을 추출하고 이를 저장하여 모델을 만든다. 모델을 만들고 나면, 모델에 입력값을 입력하여 출력값으로 예상 결과를 도출할 수 있다. 이것이 딥러닝의 동작 원리이다.

## 5.1 로지스틱 회귀의 정의
-------------
앞에서 말한 대로 입력 x에 대한 출력 y가 0 또는 1인 경우 로지스틱회귀라고 한다.    
이러한 특성을 가진 그래프는 아래와 같다. 이러한 s자 형태의 그래프를 띄는 함수를 시그모이드함수라고 한다.      
![image](https://user-images.githubusercontent.com/94752167/214230923-de089c82-8d36-4f4c-b213-432a5336bd6e.png)    

## 5.2 시그모이드 함수
-----------
시그모이드 함수를 통해 로지스틱 회귀를 풀어나가는 공식은 아래와 같다.       
![image](https://user-images.githubusercontent.com/94752167/214231043-b6e679eb-9103-4162-a9fd-a976ea03efaf.png)      

여기서도 앞에서와 같이 a와 b의 값을 알아야 함수를 특정지을 수 있다.     
- a값은 그래프의 경사도를 결정한다. a값이 커지면 경사가 커지고 a값이 작아지면 경사가 작아진다.       
![image](https://user-images.githubusercontent.com/94752167/214231284-94e180bf-cc4f-4bdc-a609-210eed9ed9a3.png)    

- b값은 그래프의 좌우이동을 의미한다. b의 값에 따라 그래프가 이동한다.        
 ![image](https://user-images.githubusercontent.com/94752167/214231391-22b525bf-7c36-41e7-b641-8eec8e033ec1.png)      
 
 - a값에 따른 오차의 변화는 아래와 같다. a값이 작아지면 오차는 무한대로 커지고 a값이 커지면 오차가 감소한다.    
 ![image](https://user-images.githubusercontent.com/94752167/214231724-1d4098f5-0043-4eb0-b661-c8a2fe3eddc1.png)    
- b값에 따른 오차의 변화는 아래와 같다. b값이 너무 2차곡선의 꼭지점을 기준으로 크거나 작어지면 오차가 증가한다.    
 ![image](https://user-images.githubusercontent.com/94752167/214231867-65661164-cb4f-4b74-b774-bdb5d383a9df.png)    

## 5.3 오차 공식
-----------
a와 b의 값은 예측값과 실제값의 차이를 오차함수를 통해 측정하고 경사하강법을 이용해 오차가 작은 방향으로 이동한다. 시그모이드 함수에서는 오차함수로 로그함수를 사용한다.    

## 5.4 로그 함수
----------
![image](https://user-images.githubusercontent.com/94752167/214232329-b25597eb-1f67-4930-ae83-830537220f26.png)    

- 위 그래프에서 파란색 선은 실제 값이 1일 때이다. 파란산은 예측값이 1일 때 오차가 0이고, 반대로 예측값이 0에 가까울수록 오차가 커진다. 해당하는 선을 식으로 표현하면 -log(h)이다.     
- 위 그래프에서 빨산색 선은 실제 값이 0일 때이다. 파란산은 예측값이 0일 때 오차가 0이고, 반대로 예측값이 1에 가까울수록 오차가 커진다. 해당하는 선을 식으로 표현하면 -log(1-h)이다.    
실제 값에 따라 두 선 중 하나의 그래프를 사용해야 하므로 아래와 같은 식으로 표현한다.    
![image](https://user-images.githubusercontent.com/94752167/214232674-ad260561-898f-403c-87fe-7b073650f8f9.png)    
- 이를 통해 도출한 오차함수는 아래와 같다.      
![image](https://user-images.githubusercontent.com/94752167/214232769-3f650a8b-07b2-4ba4-ab90-7e1025c500e9.png)     

## 5.5 코딩으로 확인하는 로지스틱 회귀
------------
https://colab.research.google.com/drive/1dlO_nA40v9RYEhu98yPrauEULHECm4hf#scrollTo=ZKbfwkmYqvh0

