# Ch.5 참 거짓 판단 장치: 로지스틱 회귀
## 1. 로지스틱 회귀의 정의
- 점들의 특성을 정확하게 담아내려면 직선이 아니라 다음과 같이 크자 형태여야 한다
- 로지스틱 회귀는 선형 회귀와 마찬가지로 적절한 선을 그려가는 과정
- 다만 직선이 아니라，참(1)과 거짓(0) 사이를 구분하는 용자 형태의 선을 그어 주는 작업

## 2. 시그모이드함수
![image](https://user-images.githubusercontent.com/101804119/214348022-fb9d421d-870c-4175-ba10-47d9785de604.png)
- 선형 회귀와 마찬가지로 ax+b를 구함
- a는 그래프의 경사도, b는 그래프의 좌우 이동을 의미
- a와 b의 값에 따라 오차가 변함.

## 3. 오차공식
- 시그모이드 함수의 특징은 y 값이 0과 1사이라는 것 
- 따라서 실제 값이 1일 때 예측 값이 0에 가까워지면 오차가 커짐 
- 반대로, 실제 값이 0일 때 예측 값이 1 에 가까워지는 경우에도 오차는 커짐 
- 이를 공식으로 만들 수 있게해 주는 함수가 로그 함수.

## 4. 로그 함수
![image](https://user-images.githubusercontent.com/101804119/214348703-1fb5ecc3-f2b6-4b31-8643-d3ce38fdd9a3.png)
- 예측 값이 0일때 오차가 없고, 1에 가까워질수롤 오차가 커짐
- 실제 값을 y_data라 할 때，이 값이 1 이면 B 부분이 없어짐, 반대로 0이면 A부분이 없어짐.

## 5. 코딩으로 확인하는 로지스틱 회귀
``` python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 공부시간 X와 합격 여부 Y의 리스트 만들기
data = [[2, 0], [4, 0], [6, 0], [8, 1], [10, 1], [12, 1], [14, 1]]

x_data = [i[0] for i in data]
y_data = [i[1] for i in data]

# 그래프로 나타내기
plt.scatter(x_data, y_data)
plt.xlim(0, 15)
plt.ylim(-.1, 1.1)

# 기울기 a와 절편 b의 값 초기화
a = 0
b = 0

# 학습률
lr = 0.05

# 시그모이드 함수 정의
def sigmoid(x):
  return 1 / (1 + np.e ** (-x))

# 경사 하강법 실행
# 1000번 반복될 때마다 각 x_data 값에 대한 현재의 a값, b값 출력
for i in range(2001):
  for x_data, y_data in data:
    a_diff = x_data*(sigmoid(a * x_data + b) - y_data)
    b_diff = sigmoid(a * x_data + b) - y_data
    a = a - lr * a_diff
    b = b - lr * b_diff
    if i % 1000 == 0:
      print("epoch=%.f, 기울기=%.04f, 절편=%.04f" % (i, a, b))

# 앞서 구한 기울기와 절편을 이용해 그래프 그리기
plt.scatter(x_data, y_data)
plt.xlim(0, 15)
plt.ylim(-.1, 1.1)
x_range = (np.arange(0, 15, 0.1)) # 그래프로 나타낼 x 값의 범위 정하기
plt.plot(np.arange(0, 15, 0.1), np.array([sigmoid(a * x + b) for x in x_range]))
plt.show()
```
