# 실습문제 답
```python
import numpy as np
x = [1, 6, 12, 18, 24]
y = [10, 30, 35, 50, 75]
mx = np.mean(x)
my = np.mean(y)
div = sum([(i - mx)**2 for i in x])
def top(x, y, mx, my):
  t = 0
  for i in range(len(x)):
    t += (x[i] - mx) * (y[i] - my)
  return t
t = top(x, y, mx, my)
a = t / div
b = my-(mx*a)
x_data = np.array(x)
y_data = np.array(y)
y_pred = a * x_data + b
print("기울기 = %.04f, 절편 = %.04f" %(a, b))
import matplotlib.pyplot as plt
plt.figure(figsize=(8,5))
plt.scatter(x, y)
plt.plot([min(x_data), max(x_data)], [min(y_pred), max(y_pred)])
plt.show()
```
![image](https://user-images.githubusercontent.com/67413252/212523275-9a49e5b8-0d92-4fac-83b6-c535209059f2.png)

# 경사하강법
오차의 그래프 - 기울기 a를 크게 잡아도, 작게 잡아도 오차가 커지며, 무한대로 갈 경우 똑같이 양쪽 모두 무한대로 가므로 이차함수 꼴로 나타낼 수 있음.      
오차가 가장 작은 점 – 이차함수의 꼭짓점 = 미분 시 기울기가 0이 되는 지점       
경사하강법 – 구해진 기울기의 반대방향으로 계속 이동시키고 미분을 구하는 과정을 반복하여 한점으로 수렴하는(꼭짓점) 오차가 최소인 지점을 찾는 방법        
학습률 – 어느만큼 이동시킬지를 정하는 것.     

$$ {1 \over n}\sum (y_i-\hat{y_i})^2 = {1 \over n}\sum (y_i-(ax_i+b))^2$$

cf. 유도과정 살펴보니 시그마를 편미분할 때는 시그마 안의 걸 그냥 바로 미분해도 괜찮은 듯/ 겉미분 속미분 사용.        
a에 대한 편미분/b에 대한 편미분       

$$ {2 \over n}\sum (ax_i+b-y_i)x_i / {2\over n}\sum(ax_i+b-y_i) $$

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
##데이터 입력
data = [[2, 81], [4, 93], [6, 91], [8, 97]]
x = [i[0] for i in data]
y = [i[1] for i in data]
##각 데이터 점으로 찍기
plt.figure(figsize=(8,5))
plt.scatter(x, y)
plt.show()
## array 객체로 변환
x_data = np.array(x)
y_data = np.array(y)
## a, b 초기값 설정
a = 0
b = 0
## 학습률, 반복횟수 설정
lr = 0.03
epochs = 2001
## 경사하강법 시작
for i in range(epochs):
  y_pred = a*x_data + b    ##prediction(예측값)의 리스트 계산
  error = y_data - y_pred  ##실측값과 예측값 오차
  a_diff = -(2/len(x_data))*sum(x_data*(error)) ##a편미분한 식 그대로 옮겨놓은것
  b_diff = -(2/len(x_data))*sum(error)          ##b편미분한 식 그대로 옮겨놓은것
  a = a - lr*a_diff        ##학습률 곱해서 a값 업데이트
  b = b - lr*b_diff        ##학습률 곱해서 a값 업데이트
  if i%100 == 0:
    print("epoch = %f, 기울기 = %.04f, 절편 = %.04f" %(i, a, b))
##도출된 값으로 그래프 그리기
y_pred = a*x_data + b
plt.scatter(x, y)
plt.plot([min(x_data), max(x_data)], [min(y_pred), max((y_pred))])
plt.show()
```
![image](https://user-images.githubusercontent.com/67413252/212522804-de10ceed-16da-4d06-8766-8dbd794f8223.png)

# 다중선형회귀 코딩
경사하강법의 공식을 똑같이 적용하면 된다. 다만 예측에 사용하는 정보의 개수가 늘어남에 따라 기울기도 여러개를 구해야 한다는 것만 주의하면 된다.
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d
#데이터 입력
data = [[2, 0, 81], [4, 4, 93], [6, 2, 91], [8, 3, 97]]
x1 = [i[0] for i in data]
x2 = [i[1] for i in data]
y = [i[2] for i in data]
#3d 데이터 시각화
ax = plt.axes(projection='3d')
ax.set_xlabel('study_hours')
ax.set_ylabel('private_class')
ax.set_zlabel('Score')
ax.dist = 11
ax.scatter(x1, x2, y)
plt.show()
#x1, x2, y를 array객체로 만들기(사칙연산 위함)
x1_data = np.array(x1)
x2_data = np.array(x2)
y_data = np.array(y)
#a1, a2, b 초기화
a1 = 0
a2 = 0
b = 0
#학습률, 반복수 지정
lr = 0.02
epochs = 2001
#경사하강법 시작
for i in range(epochs):
  y_pred = x1_data*a1 + x2_data*a2 + b    #예측값 계산
  error = y_data - y_pred                 #실측값 - 예측값 편의상 미리 저장
  a1_diff = -(2/len(x1_data)) * sum(x1_data * error)  #a1의 미분값 공식 옮겨놓음
  a2_diff = -(2/len(x2_data)) * sum(x2_data * error)  #a2의 미분값 공식 옮겨놓음
  b_diff = -(2/len(y_data)) * sum(error)  #b의 미분값 공식 옮겨놓음
  a1 = a1 - lr * a1_diff  #a1 업데이트
  a2 = a2 - lr * a2_diff  #a2 업데이트
  b = b - lr * b_diff #a3 업데이트
  if i % 100 == 0:
    print("epoch = %f, 기울기1 = %.04f, 기울기2 = %.04f, 절편 = %.04f" %(i, a1, a2, b)) #반복수, 기울기1, 기울기2, 절편 표시
```
![image](https://user-images.githubusercontent.com/67413252/212522812-68804f66-c254-4d72-8b8b-94ae4b080f6f.png)

