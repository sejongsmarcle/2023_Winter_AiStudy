## 1주차 실습 과제

### 문제
![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3a840942-cad6-4d0e-a548-69ea867cc921/Untitled.png)

[사진이 안 보인다면 Notion](https://spotless-fireman-cc9.notion.site/b8dd222f93bd491f86285e6a99c788d0)
### 풀이

### 내 코드

---

```python
#**평균값을 구할 때** **numpy라이브러리** 사용
import numpy as np 
#점선도와 일차 함수 그래프
import matplotlib.pyplot as plt

#데이터
x=[1,6,12,18,24]
y=[10,30,35,50,75]

#최소 제곱법에 사용될 평균값 구하기
x_mean=np.mean(x)
y_mean=np.mean(y)

#분모
divisor=sum([(i-x_mean)**2 for i in x])

#분자 구하기
dividend=sum([(x[i]-x_mean)*(y[i]-y_mean) for i in range(len(x))])

# 분자 구하기2- **함수 만들기**
# def top(x,y,x_mean,y_mean):
#     d=0
#     for i in range(len(x)):
#         d+=(x[i]-x_mean)*(y[i]-y_mean)
        
#     return d

# dividend=top(x,y,x_mean,y_mean)

#기울기
a=dividend/divisor

#y절편
b=y_mean-x_mean*a

print('기울기:',a)
print('y절편:',b)

#**그래프 그리기**
plt.xlabel('SMARCLE(month)')
plt.ylabel('major understanding(%)')
plotx=np.array(x)

plt.plot(plotx,a*plotx+b,c='red',label='Linear regression')
plt.scatter(x,y,c='orange',label='Data')
#plt.plot([min(x_data),max(x_data)],[min(y_pred),max(y_pred)])
#plt.plot(x,y,marker='o',linestyle='None') -> 산점도
#plt.plot(x,y,'o')

plt.legend()
plt.show()
```

### 결과

---

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/dca68e55-670e-4d70-ab29-5814f7c31426/Untitled.png)
[사진이 안 보인다면 Notion](https://spotless-fireman-cc9.notion.site/b8dd222f93bd491f86285e6a99c788d0)

### 개념

---

<aside>
💡 분명 같은 코드가 winter_AI 파일에 있고 실행이 되었는데 오류가 아닌 부분에 오류가 있다고 떴다 ㅎ… kernel만 다시 제대로 설정해주니까 되네 아놔^

</aside>

### 반복문

- 리스트 요소를 반복문을 통해 도는 방법
    - 형식: **for 변수 in 리스트 이름**
    - 변수에 리스트 요소들이 하나씩 담김.
    
    [파이썬 코딩 도장](https://dojang.io/mod/page/view.php?id=2283)
    
- 주의할 점
    - for문은 **iterable 즉, 반복문 형식으로 돌 수 있는 형태만 올 수 있음**
    
    [점프 투 파이썬](https://wikidocs.net/22)
    

### 함수 만들기

[점프 투 파이썬](https://wikidocs.net/24)

### 맷플롯립

- **xlabel : x축에 문구를 지정**
- **ylabel: y축에 문구를 지정**
- **꺾은선 그래프, plot 함수**
    - 인자에 x,y값에 대한 데이터를 넣고 이를 **전부 이은 꺾은선 그래프**
    - 일차 함수
        - 두 점만 데이터로
        - 데이터를 **array**로 설정하여 **일차식**을 넣어줌
- **점의 형태 그래프, scatter 함수**
    - c: **점의 색깔**을 지정
    - plot의 속성을 이용하여 점 형태 표시 가능
- **legend 함수**
    - **label**를 **범례**로 표시
    - 현재 그래프에 왼쪽 상단에 표시된 거
- **show 함수**
    - 그래프를 화면에 출력

[점프 투 파이썬](https://wikidocs.net/92071)

### 참고자료

---

[파이썬 선형회귀 그래프 그리기](https://blog.naver.com/PostView.nhn?blogId=monkey9327&logNo=222178014009&categoryNo=22&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView)

## 경사하강법을 이용한 단순 선형 회귀

<aside>
💡 1주차에 배운 **평균 제곱 오차**를 통해서 임의의 직선에 따라 **오차값**을 구할 수 있었다. 이제 해야 할 작업은 이러한 오차값을 구하여 **이 오차를 가장 작게 만들어 주는** a와 b를 찾아가는 작업이다.

기울기 a를 계속해서 키우면 오차도 무한대로 커지고 a가 작아져도 역시 오차가 무한대로 커진다. 이처럼 **기울기 a, y절편 b와 오차의 관계**는 **이차 함수** 그래프로 표현할 수 있다. 한 점에서의 기울기를 이용하면 **기울기가 0(미분 값이 0)인 지점에서 오차가 가장 작다.** 이런 지점을 찾기 위해 다음과 같은 과정을 반복한다.

1. 임의의 지점에서의 기울기를 구한다.
2. 기울기가 +면 음의 방향으로, -면 양의 방향으로 이동 시킨 뒤 그 지점에서의 기울기를 구한다.
3. 미분 값이 0일 때까지 해당 과정을 반복한다.

이 때, 2번 과정에서 이동 거리를 얼마나 할 것인지 결정하는 것을 **학습률**이라고 한다. 이는 딥러닝의 최적화 방법 중 하나이다. 따라서, **경사하강법은 오차의 변화에 따라 이차 함수 그래프를 만들고 ,적절한 학습률을 설정해 미분 값이 0인 지점을 구하여 오차가 가장 작은 지점으로 이동**시키는 방법인 것이다. 

평균 제곱 오차식을 이용하여 궁금한 a와 b에 대해 편미분을 한다. 계속해서 기울기를 수정하기 위해 학습률과 미분 값을 곱하여 업데이트한다.

</aside>

### 코드

```python
import numpy as np
import matplotlib.pyplot as plt

#공부 시간 x와 성적 y의 리스트
data=[[2,81],[4,93],[6,91],[8,97]]
x=[i[0] for i in data]
y=[i[1] for i in data]

#점 그래프
plt.figure(figsize=(8,5)) #그래프의 가로 세로 길이
plt.scatter(x,y)
plt.show()

#**예측값 계산 가능하게 하기 위해**
x_data=np.array(x)
y_data=np.array(y)

#기울기와 y절편 초기화
a=0
b=0

#학습률과 반복횟수 설정
lr=0.03
epochs=2001

#경사하강법 시작
for i in range(epochs):
    y_pred=a*x_data+b
    error=y_data-y_pred
    
    #오차 함수 편미분(기울기)
    a_diff=-(2/len(x_data))*sum(x_data*(error))
    b_diff=-(2/len(x_data))*sum(error)
    
    #학습률과 기울기를 통해 기울기와 y절편 업데이트
    a=a-lr*a_diff
    b=b-lr*b_diff
    
    if i%100==0:
        print("epoch=%.f,기울기=%.04f,절편=%.04f"%(i,a,b))

#최종적으로 구한 기울기와 절편을 그래프로
y_pred=a*x_data+b
plt.scatter(x,y)
plt.plot([min(x_data),max(x_data)],[min(y_pred),max(y_pred)])
plt.show()
```

## 경사하강법을 이용한 다중 선형 회귀

<aside>
💡 **변수의 개수가 2개 이상인** 다중 선형 회귀에도 **경사하강법**을 적용할 수 있다. 독립 변수가 2개라면 2개의 기울기로 나타낼 수 있고 다음과 같은 식이 나온다. 
$y=a1*x1+a2*x2+b$ 

편미분을 하면 기준이 되는 변수 외에는 상수 취급하니 미분하는 데에도 큰 문제가 없다.

</aside>

### 코드

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d

#공부 시간 x와 성적 y의 리스트
data=[[2,0,81],[4,4,93],[6,2,91],[8,3,97]]
x1=[i[0] for i in data]
x2=[i[1] for i in data]
y=[i[2] for i in data]

#점 그래프
ax=plt.axes(projection='3d')
ax.set_xlabel('study_hours')
ax.set_ylabel('private_class')
ax.set_zlabel('Score')
ax.scatter(x1,x2,y)
plt.show()

#예측값 계산 가능하게 하기 위해
x1_data=np.array(x1)
x2_data=np.array(x2)
y_data=np.array(y)

#기울기와 y절편 초기화
a1=0
a2=0
b=0

#학습률과 반복횟수 설정
lr=0.02
epochs=2001

#경사하강법 시작
for i in range(epochs):
    y_pred=a1*x1_data+a2*x2_data+b
    error=y_data-y_pred
    
    #오차 함수 편미분(기울기)
    a1_diff=-(2/len(x_data))*sum(x1_data*(error))
    a2_diff=-(2/len(x_data))*sum(x2_data*(error))
    b_diff=-(2/len(x_data))*sum(error)
    
    #학습률과 기울기를 통해 기울기와 y절편 업데이트
    a1=a1-lr*a1_diff
    a2=a2-lr*a2_diff
    b=b-lr*b_diff
    
    if i%100==0:
        print("epoch=%.f,기울기1=%.04f,기울기2=%.04f,절편=%.04f"%(i,a1,a2,b))
```
