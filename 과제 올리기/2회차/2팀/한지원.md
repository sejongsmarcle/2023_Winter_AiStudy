# Ch.4 오차 수정하기: 경사 하강법
## 1. 경사 하강법의 개요
- 경사 하강법이란
- 함수 값이 낮아지는 방향으로 독립 변수 값을 변형시켜가면서 최종적으로는 최소 함수값을 갖도록 하는 독립 변수 값을 찾는 방법이다.
- 경사 하강법은 반복적으로 기울기 a를 변화시켜서 m의 값을 찾아내는 방법을 말한다.

### 방법
1. a₁에서 미분을 구한다. 
2. 구해진 기울기의 반대 방향(기울기가 +면 음의 방향, -면 양의 방향)으로 얼 마간 이동시킨 a₂에서 미분을 구한다. 
3. 위에서 구한 미분 값이 0이 아니면 위 과정을 반복한다.

## 2. 학습률
- 딥러닝에서 학습률의 값을 적절히 바꾸면서 최적의 학습률을 찾는 것은 중요한 최적화 과정 중 하나다.
- 기울기의 부호를 바꿔 이동시킬 때 얼마만큼 이동시킬지를 신중히 결정해야 하는데, 이때 이동 거리를 정해 주는 것이 바로 학습률이다.

## 3. 코딩으로 확인하는 경사 하강법
``` python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 공부 시간 X와 성적 Y의 리스트를 만들기
data = [[2, 81], [4, 93], [6, 91], [8, 97]]
x = [i[0] for i in data]
y = [i[1] for i in data]

# 그래프로 나타내기
plt.figure(figsize=(8,5))
plt.scatter(x, y)
plt.show()

# 리스트로 되어 았는 때 y 값을 넘파이 배열로 바꾸기 (인덱스를 주어 하나씩 불러와 계산이 가능하게 하기 위함)
x_data = np.array(x)
y_data = np.array(y)

# 기울기 a와 절편 b의 값 초기화
a = 0
b = 0

# 학습률 정하기
lr = 0.03

# 몇 번 반복될지 설정
epochs = 2001

# 경사 하강법 시작
for i in range(epochs) : # 에포크 수만큼 반복
    y_pred = a * x_data + b #y를 구하는 식 세우기
    error = y_data - y_pred # 오차를 구하는 식

    # 오차 함수를 a로 미분한 값
    a_diff = -(2/len(x_data)) * sum(x_data * (error))

    # 오차 함수를 b로 미분한 값
    b_diff = - (2/len(x_data)) * sum(error)
    a = a - lr * a_diff # 학습률을 곱해 기존의 a값 업데이트
    b = b - lr * b_diff # 학습률을 곱해 기존의 b값 업데이트
    if i % 100 == 0: # 100번 반복될 때마다 현재의 a값, b값 출력
        print("epoch=%.f, 기울기=%.04f, 절편=%.04f" % (i, a, b))

# 앞서 구한 기울기와 절편을 이용해 그래프를 다시 그리기
y_pred = a * x_data + b
plt.scatter(x, y)
plt.plot([min(x_data), max(x_data)], [min(y_pred)/ max(y_pred)])
plt.show()
```

## 4. 다중 선형 회귀
- 더 정확한 예측을 하려면 추가 정보를 입력해야 한다.
- 정보를 추가해 새로운 예측 값을 구하려면 변수의 개수를 늘려 다중 선형 회귀를 만들어 주어야 한다.

## 5. 코딩으로 확인하는 다중 선형 회귀
``` python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d

# 공부 시간 X와 성적 Y의 리스트를 만들기
data = [[2, 0, 81], [4, 4, 93], [6, 2, 91], [8, 3, 97]]
x1 = [i[0] for i in data]
x2 = [i[1] for i in data]
y = [i[2] for i in data]

# 그래프로 확인
ax = plt.axes(projection='3d')
ax.set_xlabel('study_hours')
ax.set_ylabel('private_class')
ax.set_zlabel('Score')

ax.dist = 11
ax.scatter(x1, x2, y)
plt.show()

# 리스트로 되어 았는 때 y 값을 넘파이 배열로 바꾸기 (인덱스를 주어 하나씩 불러와 계산이 가능하게 하기 위함)
x1_data = np.array(x1)
x2_data = np.array(x2)
y_data = np.array(y)

# 기울기 a와 절편 b의 값 초기화
a1 = 0
a2 = 0
b = 0

# 학습률
lr = 0.02

# 몇 번 반복할지 설정(0부터 세므로 원하는 반복 횟수에 +1)
epochs = 2001

# 경사 하강법 시작
for i in range(epochs):  # epoch 수 만큼 반복
    y_pred = a1 * x1_data + a2 * x2_data + b  # y를 구하는 식 세우기
    error = y_data - y_pred  # 오차를 구하는 식
    # 오차 함수를 a1로 미분한 값
    a1_diff = -(2/len(x1_data)) * sum(x1_data * (error))
    # 오차 함수를 a2로 미분한 값
    a2_diff = -(2/len(x2_data)) * sum(x2_data * (error))
    # 오차 함수를 b로 미분한 값
    b_diff = -(2/len(x1_data)) * sum(y_data - y_pred)
    a1 = a1 - lr * a1_diff  # 학습률을 곱해 기존의 a1 값 업데이트
    a2 = a2 - lr * a2_diff  # 학습률을 곱해 기존의 a2 값 업데이트
    b = b - lr * b_diff  # 학습를을 곱해 기존의 b값 업데이트

    if i % 100 == 0: # 100번 반복될 때마다 현재의 a1, a2, b 값 출력
        print("epoch=%.f, 기울기1=%.04f, 기울기2=%.04f, 절편=%.04f" % (i, a1, a2, b))
```
