# 과제 
---------
- 문제:    
![image](https://user-images.githubusercontent.com/94752167/212787787-091518c6-f4fe-4b05-93d0-6e0fa5ab5e85.png)
      
- 풀이:
1) 그래프 식 구하기       
![image](https://user-images.githubusercontent.com/94752167/212788026-879ce220-7653-44d9-a296-a4c2a0dd4bd1.png)       
a= {(1-12.2)(10-40)+(6-12.2)(30-40)+(12-12.2)(35-40)+(18-12.2)(50-40)+(24-12.2)(75-40)}/{(1-12.2)^2+(6-12.2)^2+(12-12.2)^2+(18-12.2)^2+(24-12.2)^2}
=870/336.8=2.6    
(소수점 한자리수로 반올림)    
![image](https://user-images.githubusercontent.com/94752167/212788776-4c7ea168-1320-4d1f-bd6b-318f6328c2d2.png)       
    
b=40-(12.2*2.6)=8.28    

따라서 그래프의 식은 y=2.6x+8.28이다.

2) 그래프 그리기
https://colab.research.google.com/drive/14W5L_uFLHUhFQqwvsP7bnuuibqEMWAzQ#scrollTo=Ovfpp2A-Y0p8
     
- 결과:
![image](https://user-images.githubusercontent.com/94752167/212793155-6954744d-1c57-4941-96e9-6baef55caee8.png)    



# 4장 오차 수정하기: 경사 하강법
------------
![image](https://user-images.githubusercontent.com/94752167/212793435-b98f0bd5-0c59-421b-8b1b-18995e14b613.png)     

앞에서 기울기가 너무 큰 경우에 오차가 커진다는 것을 배웠다. 또한, 기울기가 넘우 작아도 오차가 커진다. 위 사진을 보면, 볼록한 부분 m을 기준으로 너무 a가 커지거나 작아지면 오차가 증가함을 확인할 수 있다. 따라사, 우리가 오차가 가장 작을 떄는 기울기 a가 m일 때이다. m의 값을 구하기 위해서는 임의의 한 점 a1을 찍은 후, m에 가까운 쪽으로 점점 이동하여 a1->a2->a3로 이동하는 과정이 필요하다. 이를 위해서는 각 값의 오차를 비교하여 a1보다 a2가 m에 가깝고, a2보다 a3가 m에 가깝다는 사실을 알아야 한다. 이렇게 그래프에서 오차를 비교하여 가장 작은 방향으로 이동시키는 방법이 경사하강법이다.    

## 4.1 경사 하강법의 개요
-----------

![image](https://user-images.githubusercontent.com/94752167/212793960-e8486e1d-bc80-4890-89f8-dbec90b417e1.png)    

앞에서 말한대로, 우리는 m의 위치를 찾아야 한다. 이를 위해 m에서 기울기가 0임을 이용한다. 한 점 a1에서의 기울기를 구한 후, 이 기울기와 부호는 반대이고 절대값은 감소한 값으로 a를 이동시킨다. 이 과정을 반복하여 m의 위치를 찾을 수 있다.

![image](https://user-images.githubusercontent.com/94752167/212794150-71f59cd6-63c3-4ed0-b236-d78f6d95e717.png)    

경사하강법은 이렇게 반복적으로 기울기 a를 변화시켜 m의 값을 찾아내는 방법이다.    

## 4.2 학습률
----------
기울기의 부호를 바꿔 이공시킬 때, 적절한 거리를 찾지 못하고 너무 멀리 이동시키면 아래 그림처럼 치솟는다.   

![image](https://user-images.githubusercontent.com/94752167/212794464-5145fc46-b273-460f-a245-740f98fcfc62.png)    

따라서 이동거리는 신중히 결정해야 하며, 이러한 이동거리를 정하는 것이 학습률이다. 딥러닝에서 최적의 학습률을 찾는 것은 중요한 최적화 과정 중 하나이다.    
경사 하강법은 오차의 변화에 따라 이차함수 그래프를 만들고, 적절한 학습률을 설정하여 미분 값이 0인 지점을 구하는 것이다.    

## 4.3 코딩으로 확인하는 경사 하강법
----------
최소값을 구하기 위해서는 이차함수에서 미분을 해야 하고, 이 이차함수는 평균제곱 오차를 통해 나온다. 평균제곱오차의 식은 아래와 같다.    

![image](https://user-images.githubusercontent.com/94752167/212795039-e88cd6cc-27c8-433b-ba82-77fa7dc908db.png)    
y에 식 ax+b를 대입하면 아래와 같다.    
![image](https://user-images.githubusercontent.com/94752167/212795129-e6681f10-eb39-4fd9-b135-79dc87fb619a.png)     

우리는 식을 구하기 위해 a와 b값이 필요하므로 위의 식을 각각 a와 b에 대해 편미분한다.   
![image](https://user-images.githubusercontent.com/94752167/212795218-4f93a04e-47ef-427e-bd7a-5bf8ca736054.png)

모두 정리하면 아래와 같다.    
https://colab.research.google.com/drive/1TtP9mVqUBldv9ChdBXy9SsN69ovhFsPk#scrollTo=S1XG-O7aifaD
   

## 4.4 다중 선형 회귀
--------------
선형회귀가 독립변수 x가 하나였다면, 다중선형회귀는 독립변수 x가 여러개이다.
독립변수 x가 두개인 경우, 그래프의 식은 아래와 같다.   
![image](https://user-images.githubusercontent.com/94752167/212797749-c7fbcb86-0a0c-48a7-90cb-b2e1b738845e.png)    

## 4.5 코딩으로 확인하는 다중 선형 회귀
------------
https://colab.research.google.com/drive/12898yM98OZyLjiNEmaPdpIzXHfGXHYJn#scrollTo=ZhAz0IRcnGP3








